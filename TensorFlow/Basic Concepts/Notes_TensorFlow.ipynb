{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introdution to TensorFlow for AI\n",
    "#https://www.coursera.org/learn/introduction-tensorflow/lecture/kr51q/the-hello-world-of-neural-networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 1 - Introduction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras import optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Neural Network Easy Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=keras.Sequential([keras.layers.Dense(units=1,input_shape=[1])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='sgd',loss='mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs=np.array([-1.0,0.0,1.0,2.0,3.0,4.0],dtype=float)\n",
    "ys=np.array([-3.0,-1.0,1.0,3.0,5.0,7.0],dtype=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "6/6 [==============================] - 0s 6ms/step - loss: 0.1200\n",
      "Epoch 2/500\n",
      "6/6 [==============================] - 0s 475us/step - loss: 0.1175\n",
      "Epoch 3/500\n",
      "6/6 [==============================] - 0s 229us/step - loss: 0.1151\n",
      "Epoch 4/500\n",
      "6/6 [==============================] - 0s 203us/step - loss: 0.1127\n",
      "Epoch 5/500\n",
      "6/6 [==============================] - 0s 305us/step - loss: 0.1104\n",
      "Epoch 6/500\n",
      "6/6 [==============================] - 0s 244us/step - loss: 0.1081\n",
      "Epoch 7/500\n",
      "6/6 [==============================] - 0s 769us/step - loss: 0.1059\n",
      "Epoch 8/500\n",
      "6/6 [==============================] - 0s 721us/step - loss: 0.1037\n",
      "Epoch 9/500\n",
      "6/6 [==============================] - 0s 672us/step - loss: 0.1016\n",
      "Epoch 10/500\n",
      "6/6 [==============================] - 0s 757us/step - loss: 0.0995\n",
      "Epoch 11/500\n",
      "6/6 [==============================] - 0s 325us/step - loss: 0.0975\n",
      "Epoch 12/500\n",
      "6/6 [==============================] - 0s 218us/step - loss: 0.0955\n",
      "Epoch 13/500\n",
      "6/6 [==============================] - 0s 305us/step - loss: 0.0935\n",
      "Epoch 14/500\n",
      "6/6 [==============================] - 0s 417us/step - loss: 0.0916\n",
      "Epoch 15/500\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.0897\n",
      "Epoch 16/500\n",
      "6/6 [==============================] - 0s 674us/step - loss: 0.0879\n",
      "Epoch 17/500\n",
      "6/6 [==============================] - 0s 372us/step - loss: 0.0861\n",
      "Epoch 18/500\n",
      "6/6 [==============================] - 0s 326us/step - loss: 0.0843\n",
      "Epoch 19/500\n",
      "6/6 [==============================] - 0s 310us/step - loss: 0.0826\n",
      "Epoch 20/500\n",
      "6/6 [==============================] - 0s 275us/step - loss: 0.0809\n",
      "Epoch 21/500\n",
      "6/6 [==============================] - 0s 801us/step - loss: 0.0792\n",
      "Epoch 22/500\n",
      "6/6 [==============================] - 0s 482us/step - loss: 0.0776\n",
      "Epoch 23/500\n",
      "6/6 [==============================] - 0s 684us/step - loss: 0.0760\n",
      "Epoch 24/500\n",
      "6/6 [==============================] - 0s 305us/step - loss: 0.0744\n",
      "Epoch 25/500\n",
      "6/6 [==============================] - 0s 981us/step - loss: 0.0729\n",
      "Epoch 26/500\n",
      "6/6 [==============================] - 0s 210us/step - loss: 0.0714\n",
      "Epoch 27/500\n",
      "6/6 [==============================] - 0s 521us/step - loss: 0.0699\n",
      "Epoch 28/500\n",
      "6/6 [==============================] - 0s 559us/step - loss: 0.0685\n",
      "Epoch 29/500\n",
      "6/6 [==============================] - 0s 643us/step - loss: 0.0671\n",
      "Epoch 30/500\n",
      "6/6 [==============================] - 0s 274us/step - loss: 0.0657\n",
      "Epoch 31/500\n",
      "6/6 [==============================] - 0s 644us/step - loss: 0.0644\n",
      "Epoch 32/500\n",
      "6/6 [==============================] - 0s 432us/step - loss: 0.0630\n",
      "Epoch 33/500\n",
      "6/6 [==============================] - 0s 298us/step - loss: 0.0617\n",
      "Epoch 34/500\n",
      "6/6 [==============================] - 0s 684us/step - loss: 0.0605\n",
      "Epoch 35/500\n",
      "6/6 [==============================] - 0s 343us/step - loss: 0.0592\n",
      "Epoch 36/500\n",
      "6/6 [==============================] - 0s 251us/step - loss: 0.0580\n",
      "Epoch 37/500\n",
      "6/6 [==============================] - 0s 219us/step - loss: 0.0568\n",
      "Epoch 38/500\n",
      "6/6 [==============================] - 0s 272us/step - loss: 0.0557\n",
      "Epoch 39/500\n",
      "6/6 [==============================] - 0s 371us/step - loss: 0.0545\n",
      "Epoch 40/500\n",
      "6/6 [==============================] - 0s 282us/step - loss: 0.0534\n",
      "Epoch 41/500\n",
      "6/6 [==============================] - 0s 613us/step - loss: 0.0523\n",
      "Epoch 42/500\n",
      "6/6 [==============================] - 0s 416us/step - loss: 0.0512\n",
      "Epoch 43/500\n",
      "6/6 [==============================] - 0s 485us/step - loss: 0.0502\n",
      "Epoch 44/500\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.0491\n",
      "Epoch 45/500\n",
      "6/6 [==============================] - 0s 560us/step - loss: 0.0481\n",
      "Epoch 46/500\n",
      "6/6 [==============================] - 0s 350us/step - loss: 0.0471\n",
      "Epoch 47/500\n",
      "6/6 [==============================] - 0s 436us/step - loss: 0.0462\n",
      "Epoch 48/500\n",
      "6/6 [==============================] - 0s 361us/step - loss: 0.0452\n",
      "Epoch 49/500\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.0443\n",
      "Epoch 50/500\n",
      "6/6 [==============================] - 0s 643us/step - loss: 0.0434\n",
      "Epoch 51/500\n",
      "6/6 [==============================] - 0s 214us/step - loss: 0.0425\n",
      "Epoch 52/500\n",
      "6/6 [==============================] - 0s 602us/step - loss: 0.0416\n",
      "Epoch 53/500\n",
      "6/6 [==============================] - 0s 561us/step - loss: 0.0408\n",
      "Epoch 54/500\n",
      "6/6 [==============================] - 0s 3ms/step - loss: 0.0399\n",
      "Epoch 55/500\n",
      "6/6 [==============================] - 0s 295us/step - loss: 0.0391\n",
      "Epoch 56/500\n",
      "6/6 [==============================] - 0s 424us/step - loss: 0.0383\n",
      "Epoch 57/500\n",
      "6/6 [==============================] - 0s 321us/step - loss: 0.0375\n",
      "Epoch 58/500\n",
      "6/6 [==============================] - 0s 382us/step - loss: 0.0367\n",
      "Epoch 59/500\n",
      "6/6 [==============================] - 0s 353us/step - loss: 0.0360\n",
      "Epoch 60/500\n",
      "6/6 [==============================] - 0s 509us/step - loss: 0.0353\n",
      "Epoch 61/500\n",
      "6/6 [==============================] - 0s 310us/step - loss: 0.0345\n",
      "Epoch 62/500\n",
      "6/6 [==============================] - 0s 223us/step - loss: 0.0338\n",
      "Epoch 63/500\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.0331\n",
      "Epoch 64/500\n",
      "6/6 [==============================] - 0s 559us/step - loss: 0.0324\n",
      "Epoch 65/500\n",
      "6/6 [==============================] - 0s 806us/step - loss: 0.0318\n",
      "Epoch 66/500\n",
      "6/6 [==============================] - 0s 249us/step - loss: 0.0311\n",
      "Epoch 67/500\n",
      "6/6 [==============================] - 0s 254us/step - loss: 0.0305\n",
      "Epoch 68/500\n",
      "6/6 [==============================] - 0s 199us/step - loss: 0.0299\n",
      "Epoch 69/500\n",
      "6/6 [==============================] - 0s 441us/step - loss: 0.0292\n",
      "Epoch 70/500\n",
      "6/6 [==============================] - 0s 278us/step - loss: 0.0286\n",
      "Epoch 71/500\n",
      "6/6 [==============================] - 0s 264us/step - loss: 0.0281\n",
      "Epoch 72/500\n",
      "6/6 [==============================] - 0s 466us/step - loss: 0.0275\n",
      "Epoch 73/500\n",
      "6/6 [==============================] - 0s 312us/step - loss: 0.0269\n",
      "Epoch 74/500\n",
      "6/6 [==============================] - 0s 804us/step - loss: 0.0264\n",
      "Epoch 75/500\n",
      "6/6 [==============================] - 0s 405us/step - loss: 0.0258\n",
      "Epoch 76/500\n",
      "6/6 [==============================] - 0s 257us/step - loss: 0.0253\n",
      "Epoch 77/500\n",
      "6/6 [==============================] - 0s 508us/step - loss: 0.0248\n",
      "Epoch 78/500\n",
      "6/6 [==============================] - 0s 264us/step - loss: 0.0243\n",
      "Epoch 79/500\n",
      "6/6 [==============================] - 0s 582us/step - loss: 0.0238\n",
      "Epoch 80/500\n",
      "6/6 [==============================] - 0s 289us/step - loss: 0.0233\n",
      "Epoch 81/500\n",
      "6/6 [==============================] - 0s 239us/step - loss: 0.0228\n",
      "Epoch 82/500\n",
      "6/6 [==============================] - 0s 259us/step - loss: 0.0223\n",
      "Epoch 83/500\n",
      "6/6 [==============================] - 0s 661us/step - loss: 0.0219\n",
      "Epoch 84/500\n",
      "6/6 [==============================] - 0s 212us/step - loss: 0.0214\n",
      "Epoch 85/500\n",
      "6/6 [==============================] - 0s 260us/step - loss: 0.0210\n",
      "Epoch 86/500\n",
      "6/6 [==============================] - 0s 642us/step - loss: 0.0206\n",
      "Epoch 87/500\n",
      "6/6 [==============================] - 0s 326us/step - loss: 0.0201\n",
      "Epoch 88/500\n",
      "6/6 [==============================] - 0s 364us/step - loss: 0.0197\n",
      "Epoch 89/500\n",
      "6/6 [==============================] - 0s 208us/step - loss: 0.0193\n",
      "Epoch 90/500\n",
      "6/6 [==============================] - 0s 329us/step - loss: 0.0189\n",
      "Epoch 91/500\n",
      "6/6 [==============================] - 0s 235us/step - loss: 0.0185\n",
      "Epoch 92/500\n",
      "6/6 [==============================] - 0s 549us/step - loss: 0.0181\n",
      "Epoch 93/500\n",
      "6/6 [==============================] - 0s 263us/step - loss: 0.0178\n",
      "Epoch 94/500\n",
      "6/6 [==============================] - 0s 410us/step - loss: 0.0174\n",
      "Epoch 95/500\n",
      "6/6 [==============================] - 0s 248us/step - loss: 0.0171\n",
      "Epoch 96/500\n",
      "6/6 [==============================] - 0s 968us/step - loss: 0.0167\n",
      "Epoch 97/500\n",
      "6/6 [==============================] - 0s 365us/step - loss: 0.0164\n",
      "Epoch 98/500\n",
      "6/6 [==============================] - 0s 243us/step - loss: 0.0160\n",
      "Epoch 99/500\n",
      "6/6 [==============================] - 0s 559us/step - loss: 0.0157\n",
      "Epoch 100/500\n",
      "6/6 [==============================] - 0s 223us/step - loss: 0.0154\n",
      "Epoch 101/500\n",
      "6/6 [==============================] - 0s 475us/step - loss: 0.0151\n",
      "Epoch 102/500\n",
      "6/6 [==============================] - 0s 426us/step - loss: 0.0147\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 103/500\n",
      "6/6 [==============================] - 0s 463us/step - loss: 0.0144\n",
      "Epoch 104/500\n",
      "6/6 [==============================] - 0s 783us/step - loss: 0.0141\n",
      "Epoch 105/500\n",
      "6/6 [==============================] - 0s 384us/step - loss: 0.0139\n",
      "Epoch 106/500\n",
      "6/6 [==============================] - 0s 330us/step - loss: 0.0136\n",
      "Epoch 107/500\n",
      "6/6 [==============================] - 0s 378us/step - loss: 0.0133\n",
      "Epoch 108/500\n",
      "6/6 [==============================] - 0s 308us/step - loss: 0.0130\n",
      "Epoch 109/500\n",
      "6/6 [==============================] - 0s 311us/step - loss: 0.0128\n",
      "Epoch 110/500\n",
      "6/6 [==============================] - 0s 349us/step - loss: 0.0125\n",
      "Epoch 111/500\n",
      "6/6 [==============================] - 0s 290us/step - loss: 0.0122\n",
      "Epoch 112/500\n",
      "6/6 [==============================] - 0s 329us/step - loss: 0.0120\n",
      "Epoch 113/500\n",
      "6/6 [==============================] - 0s 459us/step - loss: 0.0117\n",
      "Epoch 114/500\n",
      "6/6 [==============================] - 0s 392us/step - loss: 0.0115\n",
      "Epoch 115/500\n",
      "6/6 [==============================] - 0s 246us/step - loss: 0.0113\n",
      "Epoch 116/500\n",
      "6/6 [==============================] - 0s 618us/step - loss: 0.0110\n",
      "Epoch 117/500\n",
      "6/6 [==============================] - 0s 212us/step - loss: 0.0108\n",
      "Epoch 118/500\n",
      "6/6 [==============================] - 0s 670us/step - loss: 0.0106\n",
      "Epoch 119/500\n",
      "6/6 [==============================] - 0s 372us/step - loss: 0.0104\n",
      "Epoch 120/500\n",
      "6/6 [==============================] - 0s 574us/step - loss: 0.0101\n",
      "Epoch 121/500\n",
      "6/6 [==============================] - 0s 321us/step - loss: 0.0099\n",
      "Epoch 122/500\n",
      "6/6 [==============================] - 0s 579us/step - loss: 0.0097\n",
      "Epoch 123/500\n",
      "6/6 [==============================] - 0s 281us/step - loss: 0.0095\n",
      "Epoch 124/500\n",
      "6/6 [==============================] - 0s 273us/step - loss: 0.0093\n",
      "Epoch 125/500\n",
      "6/6 [==============================] - 0s 672us/step - loss: 0.0091\n",
      "Epoch 126/500\n",
      "6/6 [==============================] - 0s 227us/step - loss: 0.0090\n",
      "Epoch 127/500\n",
      "6/6 [==============================] - 0s 282us/step - loss: 0.0088\n",
      "Epoch 128/500\n",
      "6/6 [==============================] - 0s 275us/step - loss: 0.0086\n",
      "Epoch 129/500\n",
      "6/6 [==============================] - 0s 246us/step - loss: 0.0084\n",
      "Epoch 130/500\n",
      "6/6 [==============================] - 0s 237us/step - loss: 0.0082\n",
      "Epoch 131/500\n",
      "6/6 [==============================] - 0s 347us/step - loss: 0.0081\n",
      "Epoch 132/500\n",
      "6/6 [==============================] - 0s 273us/step - loss: 0.0079\n",
      "Epoch 133/500\n",
      "6/6 [==============================] - 0s 416us/step - loss: 0.0077\n",
      "Epoch 134/500\n",
      "6/6 [==============================] - 0s 483us/step - loss: 0.0076\n",
      "Epoch 135/500\n",
      "6/6 [==============================] - 0s 448us/step - loss: 0.0074\n",
      "Epoch 136/500\n",
      "6/6 [==============================] - 0s 595us/step - loss: 0.0073\n",
      "Epoch 137/500\n",
      "6/6 [==============================] - 0s 298us/step - loss: 0.0071\n",
      "Epoch 138/500\n",
      "6/6 [==============================] - 0s 535us/step - loss: 0.0070\n",
      "Epoch 139/500\n",
      "6/6 [==============================] - 0s 896us/step - loss: 0.0068\n",
      "Epoch 140/500\n",
      "6/6 [==============================] - 0s 333us/step - loss: 0.0067\n",
      "Epoch 141/500\n",
      "6/6 [==============================] - 0s 352us/step - loss: 0.0066\n",
      "Epoch 142/500\n",
      "6/6 [==============================] - 0s 251us/step - loss: 0.0064\n",
      "Epoch 143/500\n",
      "6/6 [==============================] - 0s 206us/step - loss: 0.0063\n",
      "Epoch 144/500\n",
      "6/6 [==============================] - 0s 187us/step - loss: 0.0062\n",
      "Epoch 145/500\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.0060\n",
      "Epoch 146/500\n",
      "6/6 [==============================] - 0s 249us/step - loss: 0.0059\n",
      "Epoch 147/500\n",
      "6/6 [==============================] - 0s 236us/step - loss: 0.0058\n",
      "Epoch 148/500\n",
      "6/6 [==============================] - 0s 554us/step - loss: 0.0057\n",
      "Epoch 149/500\n",
      "6/6 [==============================] - 0s 378us/step - loss: 0.0056\n",
      "Epoch 150/500\n",
      "6/6 [==============================] - 0s 255us/step - loss: 0.0054\n",
      "Epoch 151/500\n",
      "6/6 [==============================] - 0s 405us/step - loss: 0.0053\n",
      "Epoch 152/500\n",
      "6/6 [==============================] - 0s 299us/step - loss: 0.0052\n",
      "Epoch 153/500\n",
      "6/6 [==============================] - 0s 211us/step - loss: 0.0051\n",
      "Epoch 154/500\n",
      "6/6 [==============================] - 0s 228us/step - loss: 0.0050\n",
      "Epoch 155/500\n",
      "6/6 [==============================] - 0s 231us/step - loss: 0.0049\n",
      "Epoch 156/500\n",
      "6/6 [==============================] - 0s 243us/step - loss: 0.0048\n",
      "Epoch 157/500\n",
      "6/6 [==============================] - 0s 414us/step - loss: 0.0047\n",
      "Epoch 158/500\n",
      "6/6 [==============================] - 0s 201us/step - loss: 0.0046\n",
      "Epoch 159/500\n",
      "6/6 [==============================] - 0s 244us/step - loss: 0.0045\n",
      "Epoch 160/500\n",
      "6/6 [==============================] - 0s 290us/step - loss: 0.0044\n",
      "Epoch 161/500\n",
      "6/6 [==============================] - 0s 276us/step - loss: 0.0043\n",
      "Epoch 162/500\n",
      "6/6 [==============================] - 0s 252us/step - loss: 0.0042\n",
      "Epoch 163/500\n",
      "6/6 [==============================] - 0s 259us/step - loss: 0.0042\n",
      "Epoch 164/500\n",
      "6/6 [==============================] - 0s 239us/step - loss: 0.0041\n",
      "Epoch 165/500\n",
      "6/6 [==============================] - 0s 239us/step - loss: 0.0040\n",
      "Epoch 166/500\n",
      "6/6 [==============================] - 0s 208us/step - loss: 0.0039\n",
      "Epoch 167/500\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.0038\n",
      "Epoch 168/500\n",
      "6/6 [==============================] - 0s 245us/step - loss: 0.0037\n",
      "Epoch 169/500\n",
      "6/6 [==============================] - 0s 459us/step - loss: 0.0037\n",
      "Epoch 170/500\n",
      "6/6 [==============================] - 0s 154us/step - loss: 0.0036\n",
      "Epoch 171/500\n",
      "6/6 [==============================] - 0s 185us/step - loss: 0.0035\n",
      "Epoch 172/500\n",
      "6/6 [==============================] - 0s 400us/step - loss: 0.0034\n",
      "Epoch 173/500\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.0034\n",
      "Epoch 174/500\n",
      "6/6 [==============================] - 0s 218us/step - loss: 0.0033\n",
      "Epoch 175/500\n",
      "6/6 [==============================] - 0s 449us/step - loss: 0.0032\n",
      "Epoch 176/500\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.0032\n",
      "Epoch 177/500\n",
      "6/6 [==============================] - 0s 652us/step - loss: 0.0031\n",
      "Epoch 178/500\n",
      "6/6 [==============================] - 0s 870us/step - loss: 0.0030\n",
      "Epoch 179/500\n",
      "6/6 [==============================] - 0s 198us/step - loss: 0.0030\n",
      "Epoch 180/500\n",
      "6/6 [==============================] - 0s 396us/step - loss: 0.0029\n",
      "Epoch 181/500\n",
      "6/6 [==============================] - 0s 670us/step - loss: 0.0029\n",
      "Epoch 182/500\n",
      "6/6 [==============================] - 0s 694us/step - loss: 0.0028\n",
      "Epoch 183/500\n",
      "6/6 [==============================] - 0s 434us/step - loss: 0.0027\n",
      "Epoch 184/500\n",
      "6/6 [==============================] - 0s 669us/step - loss: 0.0027\n",
      "Epoch 185/500\n",
      "6/6 [==============================] - 0s 903us/step - loss: 0.0026\n",
      "Epoch 186/500\n",
      "6/6 [==============================] - 0s 314us/step - loss: 0.0026\n",
      "Epoch 187/500\n",
      "6/6 [==============================] - 0s 326us/step - loss: 0.0025\n",
      "Epoch 188/500\n",
      "6/6 [==============================] - 0s 145us/step - loss: 0.0025\n",
      "Epoch 189/500\n",
      "6/6 [==============================] - 0s 217us/step - loss: 0.0024\n",
      "Epoch 190/500\n",
      "6/6 [==============================] - 0s 973us/step - loss: 0.0024\n",
      "Epoch 191/500\n",
      "6/6 [==============================] - 0s 766us/step - loss: 0.0023\n",
      "Epoch 192/500\n",
      "6/6 [==============================] - 0s 815us/step - loss: 0.0023\n",
      "Epoch 193/500\n",
      "6/6 [==============================] - 0s 632us/step - loss: 0.0022\n",
      "Epoch 194/500\n",
      "6/6 [==============================] - 0s 583us/step - loss: 0.0022\n",
      "Epoch 195/500\n",
      "6/6 [==============================] - 0s 903us/step - loss: 0.0021\n",
      "Epoch 196/500\n",
      "6/6 [==============================] - 0s 415us/step - loss: 0.0021\n",
      "Epoch 197/500\n",
      "6/6 [==============================] - 0s 320us/step - loss: 0.0021\n",
      "Epoch 198/500\n",
      "6/6 [==============================] - 0s 289us/step - loss: 0.0020\n",
      "Epoch 199/500\n",
      "6/6 [==============================] - 0s 738us/step - loss: 0.0020\n",
      "Epoch 200/500\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.0019\n",
      "Epoch 201/500\n",
      "6/6 [==============================] - 0s 460us/step - loss: 0.0019\n",
      "Epoch 202/500\n",
      "6/6 [==============================] - 0s 955us/step - loss: 0.0019\n",
      "Epoch 203/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 726us/step - loss: 0.0018\n",
      "Epoch 204/500\n",
      "6/6 [==============================] - 0s 476us/step - loss: 0.0018\n",
      "Epoch 205/500\n",
      "6/6 [==============================] - 0s 364us/step - loss: 0.0017\n",
      "Epoch 206/500\n",
      "6/6 [==============================] - 0s 226us/step - loss: 0.0017\n",
      "Epoch 207/500\n",
      "6/6 [==============================] - 0s 235us/step - loss: 0.0017\n",
      "Epoch 208/500\n",
      "6/6 [==============================] - 0s 283us/step - loss: 0.0016\n",
      "Epoch 209/500\n",
      "6/6 [==============================] - 0s 227us/step - loss: 0.0016\n",
      "Epoch 210/500\n",
      "6/6 [==============================] - 0s 237us/step - loss: 0.0016\n",
      "Epoch 211/500\n",
      "6/6 [==============================] - 0s 444us/step - loss: 0.0015\n",
      "Epoch 212/500\n",
      "6/6 [==============================] - 0s 183us/step - loss: 0.0015\n",
      "Epoch 213/500\n",
      "6/6 [==============================] - 0s 214us/step - loss: 0.0015\n",
      "Epoch 214/500\n",
      "6/6 [==============================] - 0s 164us/step - loss: 0.0014\n",
      "Epoch 215/500\n",
      "6/6 [==============================] - 0s 209us/step - loss: 0.0014\n",
      "Epoch 216/500\n",
      "6/6 [==============================] - 0s 242us/step - loss: 0.0014\n",
      "Epoch 217/500\n",
      "6/6 [==============================] - 0s 245us/step - loss: 0.0014\n",
      "Epoch 218/500\n",
      "6/6 [==============================] - 0s 222us/step - loss: 0.0013\n",
      "Epoch 219/500\n",
      "6/6 [==============================] - 0s 281us/step - loss: 0.0013\n",
      "Epoch 220/500\n",
      "6/6 [==============================] - 0s 218us/step - loss: 0.0013\n",
      "Epoch 221/500\n",
      "6/6 [==============================] - 0s 196us/step - loss: 0.0012\n",
      "Epoch 222/500\n",
      "6/6 [==============================] - 0s 218us/step - loss: 0.0012\n",
      "Epoch 223/500\n",
      "6/6 [==============================] - 0s 195us/step - loss: 0.0012\n",
      "Epoch 224/500\n",
      "6/6 [==============================] - 0s 216us/step - loss: 0.0012\n",
      "Epoch 225/500\n",
      "6/6 [==============================] - 0s 238us/step - loss: 0.0011\n",
      "Epoch 226/500\n",
      "6/6 [==============================] - 0s 279us/step - loss: 0.0011\n",
      "Epoch 227/500\n",
      "6/6 [==============================] - 0s 291us/step - loss: 0.0011\n",
      "Epoch 228/500\n",
      "6/6 [==============================] - 0s 350us/step - loss: 0.0011\n",
      "Epoch 229/500\n",
      "6/6 [==============================] - 0s 392us/step - loss: 0.0011\n",
      "Epoch 230/500\n",
      "6/6 [==============================] - 0s 235us/step - loss: 0.0010\n",
      "Epoch 231/500\n",
      "6/6 [==============================] - 0s 240us/step - loss: 0.0010\n",
      "Epoch 232/500\n",
      "6/6 [==============================] - 0s 228us/step - loss: 9.9284e-04\n",
      "Epoch 233/500\n",
      "6/6 [==============================] - 0s 238us/step - loss: 9.7245e-04\n",
      "Epoch 234/500\n",
      "6/6 [==============================] - 0s 294us/step - loss: 9.5247e-04\n",
      "Epoch 235/500\n",
      "6/6 [==============================] - 0s 260us/step - loss: 9.3291e-04\n",
      "Epoch 236/500\n",
      "6/6 [==============================] - 0s 260us/step - loss: 9.1374e-04\n",
      "Epoch 237/500\n",
      "6/6 [==============================] - 0s 320us/step - loss: 8.9497e-04\n",
      "Epoch 238/500\n",
      "6/6 [==============================] - 0s 329us/step - loss: 8.7659e-04\n",
      "Epoch 239/500\n",
      "6/6 [==============================] - 0s 509us/step - loss: 8.5858e-04\n",
      "Epoch 240/500\n",
      "6/6 [==============================] - 0s 342us/step - loss: 8.4095e-04\n",
      "Epoch 241/500\n",
      "6/6 [==============================] - 0s 371us/step - loss: 8.2367e-04\n",
      "Epoch 242/500\n",
      "6/6 [==============================] - 0s 413us/step - loss: 8.0675e-04\n",
      "Epoch 243/500\n",
      "6/6 [==============================] - 0s 581us/step - loss: 7.9018e-04\n",
      "Epoch 244/500\n",
      "6/6 [==============================] - 0s 320us/step - loss: 7.7395e-04\n",
      "Epoch 245/500\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 7.5806e-04\n",
      "Epoch 246/500\n",
      "6/6 [==============================] - 0s 366us/step - loss: 7.4248e-04\n",
      "Epoch 247/500\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 7.2723e-04\n",
      "Epoch 248/500\n",
      "6/6 [==============================] - 0s 541us/step - loss: 7.1229e-04\n",
      "Epoch 249/500\n",
      "6/6 [==============================] - 0s 656us/step - loss: 6.9767e-04\n",
      "Epoch 250/500\n",
      "6/6 [==============================] - 0s 395us/step - loss: 6.8333e-04\n",
      "Epoch 251/500\n",
      "6/6 [==============================] - 0s 461us/step - loss: 6.6930e-04\n",
      "Epoch 252/500\n",
      "6/6 [==============================] - 0s 169us/step - loss: 6.5555e-04\n",
      "Epoch 253/500\n",
      "6/6 [==============================] - 0s 228us/step - loss: 6.4208e-04\n",
      "Epoch 254/500\n",
      "6/6 [==============================] - 0s 236us/step - loss: 6.2890e-04\n",
      "Epoch 255/500\n",
      "6/6 [==============================] - 0s 747us/step - loss: 6.1598e-04\n",
      "Epoch 256/500\n",
      "6/6 [==============================] - 0s 489us/step - loss: 6.0333e-04\n",
      "Epoch 257/500\n",
      "6/6 [==============================] - 0s 178us/step - loss: 5.9094e-04\n",
      "Epoch 258/500\n",
      "6/6 [==============================] - 0s 251us/step - loss: 5.7880e-04\n",
      "Epoch 259/500\n",
      "6/6 [==============================] - 0s 250us/step - loss: 5.6691e-04\n",
      "Epoch 260/500\n",
      "6/6 [==============================] - 0s 223us/step - loss: 5.5526e-04\n",
      "Epoch 261/500\n",
      "6/6 [==============================] - 0s 159us/step - loss: 5.4386e-04\n",
      "Epoch 262/500\n",
      "6/6 [==============================] - 0s 786us/step - loss: 5.3269e-04\n",
      "Epoch 263/500\n",
      "6/6 [==============================] - 0s 617us/step - loss: 5.2175e-04\n",
      "Epoch 264/500\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 5.1103e-04\n",
      "Epoch 265/500\n",
      "6/6 [==============================] - 0s 205us/step - loss: 5.0053e-04\n",
      "Epoch 266/500\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 4.9025e-04\n",
      "Epoch 267/500\n",
      "6/6 [==============================] - 0s 477us/step - loss: 4.8018e-04\n",
      "Epoch 268/500\n",
      "6/6 [==============================] - 0s 361us/step - loss: 4.7032e-04\n",
      "Epoch 269/500\n",
      "6/6 [==============================] - 0s 343us/step - loss: 4.6065e-04\n",
      "Epoch 270/500\n",
      "6/6 [==============================] - 0s 314us/step - loss: 4.5119e-04\n",
      "Epoch 271/500\n",
      "6/6 [==============================] - 0s 290us/step - loss: 4.4192e-04\n",
      "Epoch 272/500\n",
      "6/6 [==============================] - 0s 589us/step - loss: 4.3285e-04\n",
      "Epoch 273/500\n",
      "6/6 [==============================] - 0s 265us/step - loss: 4.2396e-04\n",
      "Epoch 274/500\n",
      "6/6 [==============================] - 0s 607us/step - loss: 4.1525e-04\n",
      "Epoch 275/500\n",
      "6/6 [==============================] - 0s 248us/step - loss: 4.0672e-04\n",
      "Epoch 276/500\n",
      "6/6 [==============================] - 0s 395us/step - loss: 3.9837e-04\n",
      "Epoch 277/500\n",
      "6/6 [==============================] - 0s 796us/step - loss: 3.9018e-04\n",
      "Epoch 278/500\n",
      "6/6 [==============================] - 0s 181us/step - loss: 3.8217e-04\n",
      "Epoch 279/500\n",
      "6/6 [==============================] - 0s 250us/step - loss: 3.7432e-04\n",
      "Epoch 280/500\n",
      "6/6 [==============================] - 0s 683us/step - loss: 3.6663e-04\n",
      "Epoch 281/500\n",
      "6/6 [==============================] - 0s 641us/step - loss: 3.5910e-04\n",
      "Epoch 282/500\n",
      "6/6 [==============================] - 0s 284us/step - loss: 3.5173e-04\n",
      "Epoch 283/500\n",
      "6/6 [==============================] - 0s 360us/step - loss: 3.4450e-04\n",
      "Epoch 284/500\n",
      "6/6 [==============================] - 0s 322us/step - loss: 3.3742e-04\n",
      "Epoch 285/500\n",
      "6/6 [==============================] - 0s 231us/step - loss: 3.3049e-04\n",
      "Epoch 286/500\n",
      "6/6 [==============================] - 0s 275us/step - loss: 3.2370e-04\n",
      "Epoch 287/500\n",
      "6/6 [==============================] - 0s 181us/step - loss: 3.1705e-04\n",
      "Epoch 288/500\n",
      "6/6 [==============================] - 0s 296us/step - loss: 3.1054e-04\n",
      "Epoch 289/500\n",
      "6/6 [==============================] - 0s 308us/step - loss: 3.0416e-04\n",
      "Epoch 290/500\n",
      "6/6 [==============================] - 0s 491us/step - loss: 2.9791e-04\n",
      "Epoch 291/500\n",
      "6/6 [==============================] - 0s 348us/step - loss: 2.9180e-04\n",
      "Epoch 292/500\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 2.8580e-04\n",
      "Epoch 293/500\n",
      "6/6 [==============================] - 0s 250us/step - loss: 2.7993e-04\n",
      "Epoch 294/500\n",
      "6/6 [==============================] - 0s 344us/step - loss: 2.7418e-04\n",
      "Epoch 295/500\n",
      "6/6 [==============================] - 0s 477us/step - loss: 2.6855e-04\n",
      "Epoch 296/500\n",
      "6/6 [==============================] - 0s 259us/step - loss: 2.6303e-04\n",
      "Epoch 297/500\n",
      "6/6 [==============================] - 0s 333us/step - loss: 2.5763e-04\n",
      "Epoch 298/500\n",
      "6/6 [==============================] - 0s 857us/step - loss: 2.5234e-04\n",
      "Epoch 299/500\n",
      "6/6 [==============================] - 0s 523us/step - loss: 2.4715e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 300/500\n",
      "6/6 [==============================] - 0s 290us/step - loss: 2.4208e-04\n",
      "Epoch 301/500\n",
      "6/6 [==============================] - 0s 692us/step - loss: 2.3710e-04\n",
      "Epoch 302/500\n",
      "6/6 [==============================] - 0s 271us/step - loss: 2.3223e-04\n",
      "Epoch 303/500\n",
      "6/6 [==============================] - 0s 976us/step - loss: 2.2746e-04\n",
      "Epoch 304/500\n",
      "6/6 [==============================] - 0s 424us/step - loss: 2.2279e-04\n",
      "Epoch 305/500\n",
      "6/6 [==============================] - 0s 453us/step - loss: 2.1822e-04\n",
      "Epoch 306/500\n",
      "6/6 [==============================] - 0s 637us/step - loss: 2.1373e-04\n",
      "Epoch 307/500\n",
      "6/6 [==============================] - 0s 300us/step - loss: 2.0934e-04\n",
      "Epoch 308/500\n",
      "6/6 [==============================] - 0s 393us/step - loss: 2.0504e-04\n",
      "Epoch 309/500\n",
      "6/6 [==============================] - 0s 483us/step - loss: 2.0083e-04\n",
      "Epoch 310/500\n",
      "6/6 [==============================] - 0s 670us/step - loss: 1.9671e-04\n",
      "Epoch 311/500\n",
      "6/6 [==============================] - 0s 282us/step - loss: 1.9267e-04\n",
      "Epoch 312/500\n",
      "6/6 [==============================] - 0s 554us/step - loss: 1.8871e-04\n",
      "Epoch 313/500\n",
      "6/6 [==============================] - 0s 377us/step - loss: 1.8483e-04\n",
      "Epoch 314/500\n",
      "6/6 [==============================] - 0s 236us/step - loss: 1.8103e-04\n",
      "Epoch 315/500\n",
      "6/6 [==============================] - 0s 190us/step - loss: 1.7732e-04\n",
      "Epoch 316/500\n",
      "6/6 [==============================] - 0s 260us/step - loss: 1.7367e-04\n",
      "Epoch 317/500\n",
      "6/6 [==============================] - 0s 879us/step - loss: 1.7011e-04\n",
      "Epoch 318/500\n",
      "6/6 [==============================] - 0s 374us/step - loss: 1.6661e-04\n",
      "Epoch 319/500\n",
      "6/6 [==============================] - 0s 323us/step - loss: 1.6319e-04\n",
      "Epoch 320/500\n",
      "6/6 [==============================] - 0s 273us/step - loss: 1.5984e-04\n",
      "Epoch 321/500\n",
      "6/6 [==============================] - 0s 317us/step - loss: 1.5655e-04\n",
      "Epoch 322/500\n",
      "6/6 [==============================] - 0s 981us/step - loss: 1.5334e-04\n",
      "Epoch 323/500\n",
      "6/6 [==============================] - 0s 231us/step - loss: 1.5019e-04\n",
      "Epoch 324/500\n",
      "6/6 [==============================] - 0s 515us/step - loss: 1.4710e-04\n",
      "Epoch 325/500\n",
      "6/6 [==============================] - 0s 323us/step - loss: 1.4408e-04\n",
      "Epoch 326/500\n",
      "6/6 [==============================] - 0s 222us/step - loss: 1.4112e-04\n",
      "Epoch 327/500\n",
      "6/6 [==============================] - 0s 184us/step - loss: 1.3822e-04\n",
      "Epoch 328/500\n",
      "6/6 [==============================] - 0s 366us/step - loss: 1.3538e-04\n",
      "Epoch 329/500\n",
      "6/6 [==============================] - 0s 456us/step - loss: 1.3260e-04\n",
      "Epoch 330/500\n",
      "6/6 [==============================] - 0s 342us/step - loss: 1.2988e-04\n",
      "Epoch 331/500\n",
      "6/6 [==============================] - 0s 467us/step - loss: 1.2721e-04\n",
      "Epoch 332/500\n",
      "6/6 [==============================] - 0s 148us/step - loss: 1.2460e-04\n",
      "Epoch 333/500\n",
      "6/6 [==============================] - 0s 702us/step - loss: 1.2204e-04\n",
      "Epoch 334/500\n",
      "6/6 [==============================] - 0s 287us/step - loss: 1.1953e-04\n",
      "Epoch 335/500\n",
      "6/6 [==============================] - 0s 726us/step - loss: 1.1708e-04\n",
      "Epoch 336/500\n",
      "6/6 [==============================] - 0s 276us/step - loss: 1.1467e-04\n",
      "Epoch 337/500\n",
      "6/6 [==============================] - 0s 542us/step - loss: 1.1232e-04\n",
      "Epoch 338/500\n",
      "6/6 [==============================] - 0s 209us/step - loss: 1.1001e-04\n",
      "Epoch 339/500\n",
      "6/6 [==============================] - 0s 219us/step - loss: 1.0775e-04\n",
      "Epoch 340/500\n",
      "6/6 [==============================] - 0s 337us/step - loss: 1.0554e-04\n",
      "Epoch 341/500\n",
      "6/6 [==============================] - 0s 212us/step - loss: 1.0337e-04\n",
      "Epoch 342/500\n",
      "6/6 [==============================] - 0s 205us/step - loss: 1.0125e-04\n",
      "Epoch 343/500\n",
      "6/6 [==============================] - 0s 243us/step - loss: 9.9166e-05\n",
      "Epoch 344/500\n",
      "6/6 [==============================] - 0s 223us/step - loss: 9.7129e-05\n",
      "Epoch 345/500\n",
      "6/6 [==============================] - 0s 614us/step - loss: 9.5133e-05\n",
      "Epoch 346/500\n",
      "6/6 [==============================] - 0s 246us/step - loss: 9.3178e-05\n",
      "Epoch 347/500\n",
      "6/6 [==============================] - 0s 365us/step - loss: 9.1265e-05\n",
      "Epoch 348/500\n",
      "6/6 [==============================] - 0s 561us/step - loss: 8.9390e-05\n",
      "Epoch 349/500\n",
      "6/6 [==============================] - 0s 436us/step - loss: 8.7554e-05\n",
      "Epoch 350/500\n",
      "6/6 [==============================] - 0s 255us/step - loss: 8.5755e-05\n",
      "Epoch 351/500\n",
      "6/6 [==============================] - 0s 237us/step - loss: 8.3993e-05\n",
      "Epoch 352/500\n",
      "6/6 [==============================] - 0s 206us/step - loss: 8.2268e-05\n",
      "Epoch 353/500\n",
      "6/6 [==============================] - 0s 250us/step - loss: 8.0578e-05\n",
      "Epoch 354/500\n",
      "6/6 [==============================] - 0s 483us/step - loss: 7.8923e-05\n",
      "Epoch 355/500\n",
      "6/6 [==============================] - 0s 754us/step - loss: 7.7302e-05\n",
      "Epoch 356/500\n",
      "6/6 [==============================] - 0s 634us/step - loss: 7.5714e-05\n",
      "Epoch 357/500\n",
      "6/6 [==============================] - 0s 875us/step - loss: 7.4159e-05\n",
      "Epoch 358/500\n",
      "6/6 [==============================] - 0s 437us/step - loss: 7.2636e-05\n",
      "Epoch 359/500\n",
      "6/6 [==============================] - 0s 431us/step - loss: 7.1143e-05\n",
      "Epoch 360/500\n",
      "6/6 [==============================] - 0s 437us/step - loss: 6.9683e-05\n",
      "Epoch 361/500\n",
      "6/6 [==============================] - 0s 562us/step - loss: 6.8251e-05\n",
      "Epoch 362/500\n",
      "6/6 [==============================] - 0s 418us/step - loss: 6.6850e-05\n",
      "Epoch 363/500\n",
      "6/6 [==============================] - 0s 613us/step - loss: 6.5477e-05\n",
      "Epoch 364/500\n",
      "6/6 [==============================] - 0s 447us/step - loss: 6.4132e-05\n",
      "Epoch 365/500\n",
      "6/6 [==============================] - 0s 160us/step - loss: 6.2814e-05\n",
      "Epoch 366/500\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 6.1525e-05\n",
      "Epoch 367/500\n",
      "6/6 [==============================] - 0s 518us/step - loss: 6.0260e-05\n",
      "Epoch 368/500\n",
      "6/6 [==============================] - 0s 487us/step - loss: 5.9023e-05\n",
      "Epoch 369/500\n",
      "6/6 [==============================] - 0s 458us/step - loss: 5.7811e-05\n",
      "Epoch 370/500\n",
      "6/6 [==============================] - 0s 690us/step - loss: 5.6623e-05\n",
      "Epoch 371/500\n",
      "6/6 [==============================] - 0s 464us/step - loss: 5.5461e-05\n",
      "Epoch 372/500\n",
      "6/6 [==============================] - 0s 227us/step - loss: 5.4321e-05\n",
      "Epoch 373/500\n",
      "6/6 [==============================] - 0s 317us/step - loss: 5.3205e-05\n",
      "Epoch 374/500\n",
      "6/6 [==============================] - 0s 665us/step - loss: 5.2112e-05\n",
      "Epoch 375/500\n",
      "6/6 [==============================] - 0s 525us/step - loss: 5.1042e-05\n",
      "Epoch 376/500\n",
      "6/6 [==============================] - 0s 241us/step - loss: 4.9993e-05\n",
      "Epoch 377/500\n",
      "6/6 [==============================] - 0s 468us/step - loss: 4.8967e-05\n",
      "Epoch 378/500\n",
      "6/6 [==============================] - 0s 523us/step - loss: 4.7961e-05\n",
      "Epoch 379/500\n",
      "6/6 [==============================] - 0s 221us/step - loss: 4.6977e-05\n",
      "Epoch 380/500\n",
      "6/6 [==============================] - 0s 236us/step - loss: 4.6011e-05\n",
      "Epoch 381/500\n",
      "6/6 [==============================] - 0s 358us/step - loss: 4.5066e-05\n",
      "Epoch 382/500\n",
      "6/6 [==============================] - 0s 325us/step - loss: 4.4140e-05\n",
      "Epoch 383/500\n",
      "6/6 [==============================] - 0s 370us/step - loss: 4.3234e-05\n",
      "Epoch 384/500\n",
      "6/6 [==============================] - 0s 438us/step - loss: 4.2345e-05\n",
      "Epoch 385/500\n",
      "6/6 [==============================] - 0s 226us/step - loss: 4.1476e-05\n",
      "Epoch 386/500\n",
      "6/6 [==============================] - 0s 212us/step - loss: 4.0624e-05\n",
      "Epoch 387/500\n",
      "6/6 [==============================] - 0s 219us/step - loss: 3.9790e-05\n",
      "Epoch 388/500\n",
      "6/6 [==============================] - 0s 220us/step - loss: 3.8972e-05\n",
      "Epoch 389/500\n",
      "6/6 [==============================] - 0s 639us/step - loss: 3.8171e-05\n",
      "Epoch 390/500\n",
      "6/6 [==============================] - 0s 230us/step - loss: 3.7387e-05\n",
      "Epoch 391/500\n",
      "6/6 [==============================] - 0s 358us/step - loss: 3.6619e-05\n",
      "Epoch 392/500\n",
      "6/6 [==============================] - 0s 481us/step - loss: 3.5868e-05\n",
      "Epoch 393/500\n",
      "6/6 [==============================] - 0s 235us/step - loss: 3.5130e-05\n",
      "Epoch 394/500\n",
      "6/6 [==============================] - 0s 541us/step - loss: 3.4409e-05\n",
      "Epoch 395/500\n",
      "6/6 [==============================] - 0s 238us/step - loss: 3.3702e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 396/500\n",
      "6/6 [==============================] - 0s 206us/step - loss: 3.3010e-05\n",
      "Epoch 397/500\n",
      "6/6 [==============================] - 0s 201us/step - loss: 3.2332e-05\n",
      "Epoch 398/500\n",
      "6/6 [==============================] - 0s 504us/step - loss: 3.1668e-05\n",
      "Epoch 399/500\n",
      "6/6 [==============================] - 0s 219us/step - loss: 3.1017e-05\n",
      "Epoch 400/500\n",
      "6/6 [==============================] - 0s 656us/step - loss: 3.0380e-05\n",
      "Epoch 401/500\n",
      "6/6 [==============================] - 0s 192us/step - loss: 2.9755e-05\n",
      "Epoch 402/500\n",
      "6/6 [==============================] - 0s 157us/step - loss: 2.9144e-05\n",
      "Epoch 403/500\n",
      "6/6 [==============================] - 0s 424us/step - loss: 2.8546e-05\n",
      "Epoch 404/500\n",
      "6/6 [==============================] - 0s 249us/step - loss: 2.7959e-05\n",
      "Epoch 405/500\n",
      "6/6 [==============================] - 0s 365us/step - loss: 2.7385e-05\n",
      "Epoch 406/500\n",
      "6/6 [==============================] - 0s 175us/step - loss: 2.6822e-05\n",
      "Epoch 407/500\n",
      "6/6 [==============================] - 0s 547us/step - loss: 2.6271e-05\n",
      "Epoch 408/500\n",
      "6/6 [==============================] - 0s 909us/step - loss: 2.5731e-05\n",
      "Epoch 409/500\n",
      "6/6 [==============================] - 0s 475us/step - loss: 2.5203e-05\n",
      "Epoch 410/500\n",
      "6/6 [==============================] - 0s 465us/step - loss: 2.4685e-05\n",
      "Epoch 411/500\n",
      "6/6 [==============================] - 0s 338us/step - loss: 2.4178e-05\n",
      "Epoch 412/500\n",
      "6/6 [==============================] - 0s 405us/step - loss: 2.3681e-05\n",
      "Epoch 413/500\n",
      "6/6 [==============================] - 0s 543us/step - loss: 2.3195e-05\n",
      "Epoch 414/500\n",
      "6/6 [==============================] - 0s 250us/step - loss: 2.2719e-05\n",
      "Epoch 415/500\n",
      "6/6 [==============================] - 0s 246us/step - loss: 2.2252e-05\n",
      "Epoch 416/500\n",
      "6/6 [==============================] - 0s 187us/step - loss: 2.1794e-05\n",
      "Epoch 417/500\n",
      "6/6 [==============================] - 0s 222us/step - loss: 2.1347e-05\n",
      "Epoch 418/500\n",
      "6/6 [==============================] - 0s 191us/step - loss: 2.0909e-05\n",
      "Epoch 419/500\n",
      "6/6 [==============================] - 0s 208us/step - loss: 2.0479e-05\n",
      "Epoch 420/500\n",
      "6/6 [==============================] - 0s 982us/step - loss: 2.0059e-05\n",
      "Epoch 421/500\n",
      "6/6 [==============================] - 0s 292us/step - loss: 1.9646e-05\n",
      "Epoch 422/500\n",
      "6/6 [==============================] - 0s 204us/step - loss: 1.9243e-05\n",
      "Epoch 423/500\n",
      "6/6 [==============================] - 0s 390us/step - loss: 1.8847e-05\n",
      "Epoch 424/500\n",
      "6/6 [==============================] - 0s 327us/step - loss: 1.8461e-05\n",
      "Epoch 425/500\n",
      "6/6 [==============================] - 0s 316us/step - loss: 1.8082e-05\n",
      "Epoch 426/500\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 1.7710e-05\n",
      "Epoch 427/500\n",
      "6/6 [==============================] - 0s 331us/step - loss: 1.7346e-05\n",
      "Epoch 428/500\n",
      "6/6 [==============================] - 0s 595us/step - loss: 1.6990e-05\n",
      "Epoch 429/500\n",
      "6/6 [==============================] - 0s 244us/step - loss: 1.6640e-05\n",
      "Epoch 430/500\n",
      "6/6 [==============================] - 0s 443us/step - loss: 1.6299e-05\n",
      "Epoch 431/500\n",
      "6/6 [==============================] - 0s 641us/step - loss: 1.5964e-05\n",
      "Epoch 432/500\n",
      "6/6 [==============================] - 0s 198us/step - loss: 1.5636e-05\n",
      "Epoch 433/500\n",
      "6/6 [==============================] - 0s 157us/step - loss: 1.5315e-05\n",
      "Epoch 434/500\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 1.5001e-05\n",
      "Epoch 435/500\n",
      "6/6 [==============================] - 0s 212us/step - loss: 1.4692e-05\n",
      "Epoch 436/500\n",
      "6/6 [==============================] - 0s 203us/step - loss: 1.4390e-05\n",
      "Epoch 437/500\n",
      "6/6 [==============================] - 0s 182us/step - loss: 1.4095e-05\n",
      "Epoch 438/500\n",
      "6/6 [==============================] - 0s 405us/step - loss: 1.3806e-05\n",
      "Epoch 439/500\n",
      "6/6 [==============================] - 0s 795us/step - loss: 1.3522e-05\n",
      "Epoch 440/500\n",
      "6/6 [==============================] - 0s 248us/step - loss: 1.3244e-05\n",
      "Epoch 441/500\n",
      "6/6 [==============================] - 0s 307us/step - loss: 1.2972e-05\n",
      "Epoch 442/500\n",
      "6/6 [==============================] - 0s 297us/step - loss: 1.2706e-05\n",
      "Epoch 443/500\n",
      "6/6 [==============================] - 0s 663us/step - loss: 1.2445e-05\n",
      "Epoch 444/500\n",
      "6/6 [==============================] - 0s 417us/step - loss: 1.2189e-05\n",
      "Epoch 445/500\n",
      "6/6 [==============================] - 0s 557us/step - loss: 1.1939e-05\n",
      "Epoch 446/500\n",
      "6/6 [==============================] - 0s 795us/step - loss: 1.1694e-05\n",
      "Epoch 447/500\n",
      "6/6 [==============================] - 0s 251us/step - loss: 1.1453e-05\n",
      "Epoch 448/500\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 1.1218e-05\n",
      "Epoch 449/500\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 1.0988e-05\n",
      "Epoch 450/500\n",
      "6/6 [==============================] - 0s 710us/step - loss: 1.0762e-05\n",
      "Epoch 451/500\n",
      "6/6 [==============================] - 0s 407us/step - loss: 1.0541e-05\n",
      "Epoch 452/500\n",
      "6/6 [==============================] - 0s 358us/step - loss: 1.0325e-05\n",
      "Epoch 453/500\n",
      "6/6 [==============================] - 0s 786us/step - loss: 1.0112e-05\n",
      "Epoch 454/500\n",
      "6/6 [==============================] - 0s 335us/step - loss: 9.9047e-06\n",
      "Epoch 455/500\n",
      "6/6 [==============================] - 0s 378us/step - loss: 9.7009e-06\n",
      "Epoch 456/500\n",
      "6/6 [==============================] - 0s 576us/step - loss: 9.5019e-06\n",
      "Epoch 457/500\n",
      "6/6 [==============================] - 0s 307us/step - loss: 9.3066e-06\n",
      "Epoch 458/500\n",
      "6/6 [==============================] - 0s 714us/step - loss: 9.1158e-06\n",
      "Epoch 459/500\n",
      "6/6 [==============================] - 0s 253us/step - loss: 8.9280e-06\n",
      "Epoch 460/500\n",
      "6/6 [==============================] - 0s 293us/step - loss: 8.7448e-06\n",
      "Epoch 461/500\n",
      "6/6 [==============================] - 0s 260us/step - loss: 8.5653e-06\n",
      "Epoch 462/500\n",
      "6/6 [==============================] - 0s 700us/step - loss: 8.3896e-06\n",
      "Epoch 463/500\n",
      "6/6 [==============================] - 0s 783us/step - loss: 8.2171e-06\n",
      "Epoch 464/500\n",
      "6/6 [==============================] - 0s 553us/step - loss: 8.0479e-06\n",
      "Epoch 465/500\n",
      "6/6 [==============================] - 0s 407us/step - loss: 7.8826e-06\n",
      "Epoch 466/500\n",
      "6/6 [==============================] - 0s 225us/step - loss: 7.7207e-06\n",
      "Epoch 467/500\n",
      "6/6 [==============================] - 0s 936us/step - loss: 7.5627e-06\n",
      "Epoch 468/500\n",
      "6/6 [==============================] - 0s 415us/step - loss: 7.4071e-06\n",
      "Epoch 469/500\n",
      "6/6 [==============================] - 0s 247us/step - loss: 7.2547e-06\n",
      "Epoch 470/500\n",
      "6/6 [==============================] - 0s 192us/step - loss: 7.1061e-06\n",
      "Epoch 471/500\n",
      "6/6 [==============================] - 0s 294us/step - loss: 6.9596e-06\n",
      "Epoch 472/500\n",
      "6/6 [==============================] - 0s 241us/step - loss: 6.8169e-06\n",
      "Epoch 473/500\n",
      "6/6 [==============================] - 0s 254us/step - loss: 6.6769e-06\n",
      "Epoch 474/500\n",
      "6/6 [==============================] - 0s 512us/step - loss: 6.5397e-06\n",
      "Epoch 475/500\n",
      "6/6 [==============================] - 0s 246us/step - loss: 6.4054e-06\n",
      "Epoch 476/500\n",
      "6/6 [==============================] - 0s 391us/step - loss: 6.2737e-06\n",
      "Epoch 477/500\n",
      "6/6 [==============================] - 0s 243us/step - loss: 6.1449e-06\n",
      "Epoch 478/500\n",
      "6/6 [==============================] - 0s 306us/step - loss: 6.0182e-06\n",
      "Epoch 479/500\n",
      "6/6 [==============================] - 0s 239us/step - loss: 5.8946e-06\n",
      "Epoch 480/500\n",
      "6/6 [==============================] - 0s 243us/step - loss: 5.7737e-06\n",
      "Epoch 481/500\n",
      "6/6 [==============================] - 0s 274us/step - loss: 5.6552e-06\n",
      "Epoch 482/500\n",
      "6/6 [==============================] - 0s 221us/step - loss: 5.5387e-06\n",
      "Epoch 483/500\n",
      "6/6 [==============================] - 0s 247us/step - loss: 5.4251e-06\n",
      "Epoch 484/500\n",
      "6/6 [==============================] - 0s 237us/step - loss: 5.3134e-06\n",
      "Epoch 485/500\n",
      "6/6 [==============================] - 0s 275us/step - loss: 5.2042e-06\n",
      "Epoch 486/500\n",
      "6/6 [==============================] - 0s 232us/step - loss: 5.0975e-06\n",
      "Epoch 487/500\n",
      "6/6 [==============================] - 0s 232us/step - loss: 4.9928e-06\n",
      "Epoch 488/500\n",
      "6/6 [==============================] - 0s 236us/step - loss: 4.8902e-06\n",
      "Epoch 489/500\n",
      "6/6 [==============================] - 0s 299us/step - loss: 4.7900e-06\n",
      "Epoch 490/500\n",
      "6/6 [==============================] - 0s 278us/step - loss: 4.6916e-06\n",
      "Epoch 491/500\n",
      "6/6 [==============================] - 0s 285us/step - loss: 4.5950e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 492/500\n",
      "6/6 [==============================] - 0s 251us/step - loss: 4.5008e-06\n",
      "Epoch 493/500\n",
      "6/6 [==============================] - 0s 263us/step - loss: 4.4082e-06\n",
      "Epoch 494/500\n",
      "6/6 [==============================] - 0s 431us/step - loss: 4.3174e-06\n",
      "Epoch 495/500\n",
      "6/6 [==============================] - 0s 237us/step - loss: 4.2290e-06\n",
      "Epoch 496/500\n",
      "6/6 [==============================] - 0s 242us/step - loss: 4.1421e-06\n",
      "Epoch 497/500\n",
      "6/6 [==============================] - 0s 282us/step - loss: 4.0569e-06\n",
      "Epoch 498/500\n",
      "6/6 [==============================] - 0s 531us/step - loss: 3.9736e-06\n",
      "Epoch 499/500\n",
      "6/6 [==============================] - 0s 522us/step - loss: 3.8922e-06\n",
      "Epoch 500/500\n",
      "6/6 [==============================] - 0s 445us/step - loss: 3.8121e-06\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x133c6fac8>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(xs,ys,epochs=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 会接近19但不会是19的原因"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "首先是您使用很少的数据对其进行了训练。只有六点。这六个点是线性的，但是不能保证每个X的关系都是Y等于2X减去1。对于X等于10，Y等于19的可能性很高，但是神经网络不是正数。因此，它将得出Y的实际值。这是第二个主要原因。当使用神经网络时，由于他们试图找出所有问题的答案，所以他们要处理概率。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might have thought 19, right? But it ended up being a little under. Why do you think that is? \n",
    "\n",
    "Remember that neural networks deal with probabilities, so given the data that we fed the NN with, it calculated that there is a very high probability that the relationship between X and Y is Y=2X-1, but with only 6 data points we can't know for sure. As a result, the result for 10 is very close to 19, but not necessarily 19. \n",
    "\n",
    "As you work with neural networks, you'll see this pattern recurring. You will almost always deal with probabilities, not certainties, and will do a little bit of coding to figure out what the result is based on the probabilities, particularly when it comes to classification.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[18.994303]]\n"
     ]
    }
   ],
   "source": [
    "print(model.predict([10.0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Exerice - Predict House Price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/kristy/anaconda3/lib/python3.6/site-packages/tensorflow/python/keras/utils/losses_utils.py:170: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/500\n",
      "12/12 [==============================] - 0s 12ms/sample - loss: 190466.0000\n",
      "Epoch 2/500\n",
      "12/12 [==============================] - 0s 153us/sample - loss: 17404.0117\n",
      "Epoch 3/500\n",
      "12/12 [==============================] - 0s 165us/sample - loss: 1963.1996\n",
      "Epoch 4/500\n",
      "12/12 [==============================] - 0s 164us/sample - loss: 582.0678\n",
      "Epoch 5/500\n",
      "12/12 [==============================] - 0s 138us/sample - loss: 455.0786\n",
      "Epoch 6/500\n",
      "12/12 [==============================] - 0s 350us/sample - loss: 439.9937\n",
      "Epoch 7/500\n",
      "12/12 [==============================] - 0s 253us/sample - loss: 434.9254\n",
      "Epoch 8/500\n",
      "12/12 [==============================] - 0s 346us/sample - loss: 430.7848\n",
      "Epoch 9/500\n",
      "12/12 [==============================] - 0s 338us/sample - loss: 426.7618\n",
      "Epoch 10/500\n",
      "12/12 [==============================] - 0s 262us/sample - loss: 422.7832\n",
      "Epoch 11/500\n",
      "12/12 [==============================] - 0s 293us/sample - loss: 418.8424\n",
      "Epoch 12/500\n",
      "12/12 [==============================] - 0s 504us/sample - loss: 414.9381\n",
      "Epoch 13/500\n",
      "12/12 [==============================] - 0s 388us/sample - loss: 411.0706\n",
      "Epoch 14/500\n",
      "12/12 [==============================] - 0s 300us/sample - loss: 407.2391\n",
      "Epoch 15/500\n",
      "12/12 [==============================] - 0s 270us/sample - loss: 403.4433\n",
      "Epoch 16/500\n",
      "12/12 [==============================] - 0s 505us/sample - loss: 399.6826\n",
      "Epoch 17/500\n",
      "12/12 [==============================] - 0s 420us/sample - loss: 395.9570\n",
      "Epoch 18/500\n",
      "12/12 [==============================] - 0s 357us/sample - loss: 392.2660\n",
      "Epoch 19/500\n",
      "12/12 [==============================] - 0s 637us/sample - loss: 388.6097\n",
      "Epoch 20/500\n",
      "12/12 [==============================] - 0s 275us/sample - loss: 384.9877\n",
      "Epoch 21/500\n",
      "12/12 [==============================] - 0s 225us/sample - loss: 381.3988\n",
      "Epoch 22/500\n",
      "12/12 [==============================] - 0s 491us/sample - loss: 377.8440\n",
      "Epoch 23/500\n",
      "12/12 [==============================] - 0s 266us/sample - loss: 374.3220\n",
      "Epoch 24/500\n",
      "12/12 [==============================] - 0s 451us/sample - loss: 370.8331\n",
      "Epoch 25/500\n",
      "12/12 [==============================] - 0s 233us/sample - loss: 367.3761\n",
      "Epoch 26/500\n",
      "12/12 [==============================] - 0s 541us/sample - loss: 363.9522\n",
      "Epoch 27/500\n",
      "12/12 [==============================] - 0s 489us/sample - loss: 360.5595\n",
      "Epoch 28/500\n",
      "12/12 [==============================] - 0s 601us/sample - loss: 357.1988\n",
      "Epoch 29/500\n",
      "12/12 [==============================] - 0s 246us/sample - loss: 353.8693\n",
      "Epoch 30/500\n",
      "12/12 [==============================] - 0s 607us/sample - loss: 350.5710\n",
      "Epoch 31/500\n",
      "12/12 [==============================] - 0s 423us/sample - loss: 347.3028\n",
      "Epoch 32/500\n",
      "12/12 [==============================] - 0s 252us/sample - loss: 344.0658\n",
      "Epoch 33/500\n",
      "12/12 [==============================] - 0s 807us/sample - loss: 340.8587\n",
      "Epoch 34/500\n",
      "12/12 [==============================] - 0s 223us/sample - loss: 337.6817\n",
      "Epoch 35/500\n",
      "12/12 [==============================] - 0s 332us/sample - loss: 334.5343\n",
      "Epoch 36/500\n",
      "12/12 [==============================] - 0s 128us/sample - loss: 331.4157\n",
      "Epoch 37/500\n",
      "12/12 [==============================] - 0s 210us/sample - loss: 328.3268\n",
      "Epoch 38/500\n",
      "12/12 [==============================] - 0s 284us/sample - loss: 325.2662\n",
      "Epoch 39/500\n",
      "12/12 [==============================] - 0s 389us/sample - loss: 322.2346\n",
      "Epoch 40/500\n",
      "12/12 [==============================] - 0s 450us/sample - loss: 319.2305\n",
      "Epoch 41/500\n",
      "12/12 [==============================] - 0s 179us/sample - loss: 316.2551\n",
      "Epoch 42/500\n",
      "12/12 [==============================] - 0s 176us/sample - loss: 313.3074\n",
      "Epoch 43/500\n",
      "12/12 [==============================] - 0s 374us/sample - loss: 310.3868\n",
      "Epoch 44/500\n",
      "12/12 [==============================] - 0s 230us/sample - loss: 307.4938\n",
      "Epoch 45/500\n",
      "12/12 [==============================] - 0s 554us/sample - loss: 304.6275\n",
      "Epoch 46/500\n",
      "12/12 [==============================] - 0s 368us/sample - loss: 301.7881\n",
      "Epoch 47/500\n",
      "12/12 [==============================] - 0s 222us/sample - loss: 298.9754\n",
      "Epoch 48/500\n",
      "12/12 [==============================] - 0s 200us/sample - loss: 296.1886\n",
      "Epoch 49/500\n",
      "12/12 [==============================] - 0s 213us/sample - loss: 293.4276\n",
      "Epoch 50/500\n",
      "12/12 [==============================] - 0s 445us/sample - loss: 290.6925\n",
      "Epoch 51/500\n",
      "12/12 [==============================] - 0s 180us/sample - loss: 287.9829\n",
      "Epoch 52/500\n",
      "12/12 [==============================] - 0s 214us/sample - loss: 285.2984\n",
      "Epoch 53/500\n",
      "12/12 [==============================] - 0s 224us/sample - loss: 282.6391\n",
      "Epoch 54/500\n",
      "12/12 [==============================] - 0s 128us/sample - loss: 280.0047\n",
      "Epoch 55/500\n",
      "12/12 [==============================] - 0s 109us/sample - loss: 277.3948\n",
      "Epoch 56/500\n",
      "12/12 [==============================] - 0s 115us/sample - loss: 274.8092\n",
      "Epoch 57/500\n",
      "12/12 [==============================] - 0s 140us/sample - loss: 272.2477\n",
      "Epoch 58/500\n",
      "12/12 [==============================] - 0s 159us/sample - loss: 269.7097\n",
      "Epoch 59/500\n",
      "12/12 [==============================] - 0s 203us/sample - loss: 267.1960\n",
      "Epoch 60/500\n",
      "12/12 [==============================] - 0s 206us/sample - loss: 264.7054\n",
      "Epoch 61/500\n",
      "12/12 [==============================] - 0s 161us/sample - loss: 262.2380\n",
      "Epoch 62/500\n",
      "12/12 [==============================] - 0s 266us/sample - loss: 259.7939\n",
      "Epoch 63/500\n",
      "12/12 [==============================] - 0s 186us/sample - loss: 257.3721\n",
      "Epoch 64/500\n",
      "12/12 [==============================] - 0s 181us/sample - loss: 254.9731\n",
      "Epoch 65/500\n",
      "12/12 [==============================] - 0s 271us/sample - loss: 252.5964\n",
      "Epoch 66/500\n",
      "12/12 [==============================] - 0s 555us/sample - loss: 250.2419\n",
      "Epoch 67/500\n",
      "12/12 [==============================] - 0s 232us/sample - loss: 247.9097\n",
      "Epoch 68/500\n",
      "12/12 [==============================] - 0s 255us/sample - loss: 245.5986\n",
      "Epoch 69/500\n",
      "12/12 [==============================] - 0s 332us/sample - loss: 243.3092\n",
      "Epoch 70/500\n",
      "12/12 [==============================] - 0s 542us/sample - loss: 241.0415\n",
      "Epoch 71/500\n",
      "12/12 [==============================] - 0s 233us/sample - loss: 238.7947\n",
      "Epoch 72/500\n",
      "12/12 [==============================] - 0s 240us/sample - loss: 236.5690\n",
      "Epoch 73/500\n",
      "12/12 [==============================] - 0s 212us/sample - loss: 234.3638\n",
      "Epoch 74/500\n",
      "12/12 [==============================] - 0s 388us/sample - loss: 232.1793\n",
      "Epoch 75/500\n",
      "12/12 [==============================] - 0s 256us/sample - loss: 230.0151\n",
      "Epoch 76/500\n",
      "12/12 [==============================] - 0s 156us/sample - loss: 227.8708\n",
      "Epoch 77/500\n",
      "12/12 [==============================] - 0s 212us/sample - loss: 225.7471\n",
      "Epoch 78/500\n",
      "12/12 [==============================] - 0s 111us/sample - loss: 223.6429\n",
      "Epoch 79/500\n",
      "12/12 [==============================] - 0s 151us/sample - loss: 221.5581\n",
      "Epoch 80/500\n",
      "12/12 [==============================] - 0s 481us/sample - loss: 219.4932\n",
      "Epoch 81/500\n",
      "12/12 [==============================] - 0s 355us/sample - loss: 217.4472\n",
      "Epoch 82/500\n",
      "12/12 [==============================] - 0s 148us/sample - loss: 215.4201\n",
      "Epoch 83/500\n",
      "12/12 [==============================] - 0s 444us/sample - loss: 213.4124\n",
      "Epoch 84/500\n",
      "12/12 [==============================] - 0s 174us/sample - loss: 211.4232\n",
      "Epoch 85/500\n",
      "12/12 [==============================] - 0s 108us/sample - loss: 209.4525\n",
      "Epoch 86/500\n",
      "12/12 [==============================] - 0s 172us/sample - loss: 207.5001\n",
      "Epoch 87/500\n",
      "12/12 [==============================] - 0s 152us/sample - loss: 205.5660\n",
      "Epoch 88/500\n",
      "12/12 [==============================] - 0s 284us/sample - loss: 203.6498\n",
      "Epoch 89/500\n",
      "12/12 [==============================] - 0s 213us/sample - loss: 201.7514\n",
      "Epoch 90/500\n",
      "12/12 [==============================] - 0s 379us/sample - loss: 199.8708\n",
      "Epoch 91/500\n",
      "12/12 [==============================] - 0s 148us/sample - loss: 198.0079\n",
      "Epoch 92/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12/12 [==============================] - 0s 94us/sample - loss: 196.1623\n",
      "Epoch 93/500\n",
      "12/12 [==============================] - 0s 443us/sample - loss: 194.3339\n",
      "Epoch 94/500\n",
      "12/12 [==============================] - 0s 207us/sample - loss: 192.5226\n",
      "Epoch 95/500\n",
      "12/12 [==============================] - 0s 887us/sample - loss: 190.7278\n",
      "Epoch 96/500\n",
      "12/12 [==============================] - 0s 140us/sample - loss: 188.9501\n",
      "Epoch 97/500\n",
      "12/12 [==============================] - 0s 348us/sample - loss: 187.1888\n",
      "Epoch 98/500\n",
      "12/12 [==============================] - 0s 374us/sample - loss: 185.4441\n",
      "Epoch 99/500\n",
      "12/12 [==============================] - 0s 275us/sample - loss: 183.7156\n",
      "Epoch 100/500\n",
      "12/12 [==============================] - 0s 262us/sample - loss: 182.0029\n",
      "Epoch 101/500\n",
      "12/12 [==============================] - 0s 155us/sample - loss: 180.3065\n",
      "Epoch 102/500\n",
      "12/12 [==============================] - 0s 207us/sample - loss: 178.6260\n",
      "Epoch 103/500\n",
      "12/12 [==============================] - 0s 236us/sample - loss: 176.9610\n",
      "Epoch 104/500\n",
      "12/12 [==============================] - 0s 299us/sample - loss: 175.3113\n",
      "Epoch 105/500\n",
      "12/12 [==============================] - 0s 270us/sample - loss: 173.6775\n",
      "Epoch 106/500\n",
      "12/12 [==============================] - 0s 229us/sample - loss: 172.0583\n",
      "Epoch 107/500\n",
      "12/12 [==============================] - 0s 372us/sample - loss: 170.4545\n",
      "Epoch 108/500\n",
      "12/12 [==============================] - 0s 664us/sample - loss: 168.8661\n",
      "Epoch 109/500\n",
      "12/12 [==============================] - 0s 229us/sample - loss: 167.2918\n",
      "Epoch 110/500\n",
      "12/12 [==============================] - 0s 139us/sample - loss: 165.7324\n",
      "Epoch 111/500\n",
      "12/12 [==============================] - 0s 218us/sample - loss: 164.1877\n",
      "Epoch 112/500\n",
      "12/12 [==============================] - 0s 496us/sample - loss: 162.6571\n",
      "Epoch 113/500\n",
      "12/12 [==============================] - 0s 286us/sample - loss: 161.1412\n",
      "Epoch 114/500\n",
      "12/12 [==============================] - 0s 524us/sample - loss: 159.6390\n",
      "Epoch 115/500\n",
      "12/12 [==============================] - 0s 290us/sample - loss: 158.1510\n",
      "Epoch 116/500\n",
      "12/12 [==============================] - 0s 434us/sample - loss: 156.6770\n",
      "Epoch 117/500\n",
      "12/12 [==============================] - 0s 384us/sample - loss: 155.2167\n",
      "Epoch 118/500\n",
      "12/12 [==============================] - 0s 203us/sample - loss: 153.7697\n",
      "Epoch 119/500\n",
      "12/12 [==============================] - 0s 362us/sample - loss: 152.3364\n",
      "Epoch 120/500\n",
      "12/12 [==============================] - 0s 240us/sample - loss: 150.9166\n",
      "Epoch 121/500\n",
      "12/12 [==============================] - 0s 383us/sample - loss: 149.5099\n",
      "Epoch 122/500\n",
      "12/12 [==============================] - 0s 224us/sample - loss: 148.1162\n",
      "Epoch 123/500\n",
      "12/12 [==============================] - 0s 194us/sample - loss: 146.7356\n",
      "Epoch 124/500\n",
      "12/12 [==============================] - 0s 469us/sample - loss: 145.3679\n",
      "Epoch 125/500\n",
      "12/12 [==============================] - 0s 444us/sample - loss: 144.0128\n",
      "Epoch 126/500\n",
      "12/12 [==============================] - 0s 585us/sample - loss: 142.6706\n",
      "Epoch 127/500\n",
      "12/12 [==============================] - 0s 216us/sample - loss: 141.3406\n",
      "Epoch 128/500\n",
      "12/12 [==============================] - 0s 199us/sample - loss: 140.0232\n",
      "Epoch 129/500\n",
      "12/12 [==============================] - 0s 496us/sample - loss: 138.7180\n",
      "Epoch 130/500\n",
      "12/12 [==============================] - 0s 293us/sample - loss: 137.4250\n",
      "Epoch 131/500\n",
      "12/12 [==============================] - 0s 164us/sample - loss: 136.1440\n",
      "Epoch 132/500\n",
      "12/12 [==============================] - 0s 346us/sample - loss: 134.8750\n",
      "Epoch 133/500\n",
      "12/12 [==============================] - 0s 273us/sample - loss: 133.6179\n",
      "Epoch 134/500\n",
      "12/12 [==============================] - 0s 224us/sample - loss: 132.3723\n",
      "Epoch 135/500\n",
      "12/12 [==============================] - 0s 250us/sample - loss: 131.1387\n",
      "Epoch 136/500\n",
      "12/12 [==============================] - 0s 261us/sample - loss: 129.9160\n",
      "Epoch 137/500\n",
      "12/12 [==============================] - 0s 297us/sample - loss: 128.7053\n",
      "Epoch 138/500\n",
      "12/12 [==============================] - 0s 202us/sample - loss: 127.5057\n",
      "Epoch 139/500\n",
      "12/12 [==============================] - 0s 274us/sample - loss: 126.3170\n",
      "Epoch 140/500\n",
      "12/12 [==============================] - 0s 440us/sample - loss: 125.1397\n",
      "Epoch 141/500\n",
      "12/12 [==============================] - 0s 246us/sample - loss: 123.9734\n",
      "Epoch 142/500\n",
      "12/12 [==============================] - 0s 166us/sample - loss: 122.8174\n",
      "Epoch 143/500\n",
      "12/12 [==============================] - 0s 537us/sample - loss: 121.6728\n",
      "Epoch 144/500\n",
      "12/12 [==============================] - 0s 198us/sample - loss: 120.5388\n",
      "Epoch 145/500\n",
      "12/12 [==============================] - 0s 448us/sample - loss: 119.4151\n",
      "Epoch 146/500\n",
      "12/12 [==============================] - 0s 186us/sample - loss: 118.3021\n",
      "Epoch 147/500\n",
      "12/12 [==============================] - 0s 249us/sample - loss: 117.1992\n",
      "Epoch 148/500\n",
      "12/12 [==============================] - 0s 496us/sample - loss: 116.1070\n",
      "Epoch 149/500\n",
      "12/12 [==============================] - 0s 188us/sample - loss: 115.0246\n",
      "Epoch 150/500\n",
      "12/12 [==============================] - 0s 322us/sample - loss: 113.9523\n",
      "Epoch 151/500\n",
      "12/12 [==============================] - 0s 179us/sample - loss: 112.8903\n",
      "Epoch 152/500\n",
      "12/12 [==============================] - 0s 159us/sample - loss: 111.8382\n",
      "Epoch 153/500\n",
      "12/12 [==============================] - 0s 254us/sample - loss: 110.7955\n",
      "Epoch 154/500\n",
      "12/12 [==============================] - 0s 224us/sample - loss: 109.7627\n",
      "Epoch 155/500\n",
      "12/12 [==============================] - 0s 248us/sample - loss: 108.7398\n",
      "Epoch 156/500\n",
      "12/12 [==============================] - 0s 161us/sample - loss: 107.7261\n",
      "Epoch 157/500\n",
      "12/12 [==============================] - 0s 176us/sample - loss: 106.7220\n",
      "Epoch 158/500\n",
      "12/12 [==============================] - 0s 200us/sample - loss: 105.7274\n",
      "Epoch 159/500\n",
      "12/12 [==============================] - 0s 210us/sample - loss: 104.7417\n",
      "Epoch 160/500\n",
      "12/12 [==============================] - 0s 316us/sample - loss: 103.7653\n",
      "Epoch 161/500\n",
      "12/12 [==============================] - 0s 265us/sample - loss: 102.7983\n",
      "Epoch 162/500\n",
      "12/12 [==============================] - 0s 259us/sample - loss: 101.8401\n",
      "Epoch 163/500\n",
      "12/12 [==============================] - 0s 213us/sample - loss: 100.8908\n",
      "Epoch 164/500\n",
      "12/12 [==============================] - 0s 192us/sample - loss: 99.9503\n",
      "Epoch 165/500\n",
      "12/12 [==============================] - 0s 338us/sample - loss: 99.0187\n",
      "Epoch 166/500\n",
      "12/12 [==============================] - 0s 446us/sample - loss: 98.0958\n",
      "Epoch 167/500\n",
      "12/12 [==============================] - 0s 141us/sample - loss: 97.1815\n",
      "Epoch 168/500\n",
      "12/12 [==============================] - 0s 189us/sample - loss: 96.2757\n",
      "Epoch 169/500\n",
      "12/12 [==============================] - 0s 181us/sample - loss: 95.3782\n",
      "Epoch 170/500\n",
      "12/12 [==============================] - 0s 389us/sample - loss: 94.4892\n",
      "Epoch 171/500\n",
      "12/12 [==============================] - 0s 164us/sample - loss: 93.6084\n",
      "Epoch 172/500\n",
      "12/12 [==============================] - 0s 165us/sample - loss: 92.7359\n",
      "Epoch 173/500\n",
      "12/12 [==============================] - 0s 125us/sample - loss: 91.8715\n",
      "Epoch 174/500\n",
      "12/12 [==============================] - 0s 331us/sample - loss: 91.0151\n",
      "Epoch 175/500\n",
      "12/12 [==============================] - 0s 197us/sample - loss: 90.1668\n",
      "Epoch 176/500\n",
      "12/12 [==============================] - 0s 395us/sample - loss: 89.3264\n",
      "Epoch 177/500\n",
      "12/12 [==============================] - 0s 183us/sample - loss: 88.4938\n",
      "Epoch 178/500\n",
      "12/12 [==============================] - 0s 250us/sample - loss: 87.6687\n",
      "Epoch 179/500\n",
      "12/12 [==============================] - 0s 218us/sample - loss: 86.8515\n",
      "Epoch 180/500\n",
      "12/12 [==============================] - 0s 157us/sample - loss: 86.0422\n",
      "Epoch 181/500\n",
      "12/12 [==============================] - 0s 247us/sample - loss: 85.2401\n",
      "Epoch 182/500\n",
      "12/12 [==============================] - 0s 354us/sample - loss: 84.4454\n",
      "Epoch 183/500\n",
      "12/12 [==============================] - 0s 118us/sample - loss: 83.6584\n",
      "Epoch 184/500\n",
      "12/12 [==============================] - 0s 255us/sample - loss: 82.8786\n",
      "Epoch 185/500\n",
      "12/12 [==============================] - 0s 225us/sample - loss: 82.1061\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 186/500\n",
      "12/12 [==============================] - 0s 135us/sample - loss: 81.3406\n",
      "Epoch 187/500\n",
      "12/12 [==============================] - 0s 331us/sample - loss: 80.5825\n",
      "Epoch 188/500\n",
      "12/12 [==============================] - 0s 282us/sample - loss: 79.8313\n",
      "Epoch 189/500\n",
      "12/12 [==============================] - 0s 104us/sample - loss: 79.0874\n",
      "Epoch 190/500\n",
      "12/12 [==============================] - 0s 115us/sample - loss: 78.3501\n",
      "Epoch 191/500\n",
      "12/12 [==============================] - 0s 210us/sample - loss: 77.6197\n",
      "Epoch 192/500\n",
      "12/12 [==============================] - 0s 185us/sample - loss: 76.8962\n",
      "Epoch 193/500\n",
      "12/12 [==============================] - 0s 142us/sample - loss: 76.1796\n",
      "Epoch 194/500\n",
      "12/12 [==============================] - 0s 178us/sample - loss: 75.4695\n",
      "Epoch 195/500\n",
      "12/12 [==============================] - 0s 190us/sample - loss: 74.7659\n",
      "Epoch 196/500\n",
      "12/12 [==============================] - 0s 190us/sample - loss: 74.0691\n",
      "Epoch 197/500\n",
      "12/12 [==============================] - 0s 164us/sample - loss: 73.3788\n",
      "Epoch 198/500\n",
      "12/12 [==============================] - 0s 195us/sample - loss: 72.6948\n",
      "Epoch 199/500\n",
      "12/12 [==============================] - 0s 397us/sample - loss: 72.0171\n",
      "Epoch 200/500\n",
      "12/12 [==============================] - 0s 147us/sample - loss: 71.3458\n",
      "Epoch 201/500\n",
      "12/12 [==============================] - 0s 173us/sample - loss: 70.6809\n",
      "Epoch 202/500\n",
      "12/12 [==============================] - 0s 148us/sample - loss: 70.0219\n",
      "Epoch 203/500\n",
      "12/12 [==============================] - 0s 161us/sample - loss: 69.3695\n",
      "Epoch 204/500\n",
      "12/12 [==============================] - 0s 266us/sample - loss: 68.7228\n",
      "Epoch 205/500\n",
      "12/12 [==============================] - 0s 155us/sample - loss: 68.0822\n",
      "Epoch 206/500\n",
      "12/12 [==============================] - 0s 120us/sample - loss: 67.4476\n",
      "Epoch 207/500\n",
      "12/12 [==============================] - 0s 169us/sample - loss: 66.8190\n",
      "Epoch 208/500\n",
      "12/12 [==============================] - 0s 168us/sample - loss: 66.1962\n",
      "Epoch 209/500\n",
      "12/12 [==============================] - 0s 176us/sample - loss: 65.5790\n",
      "Epoch 210/500\n",
      "12/12 [==============================] - 0s 117us/sample - loss: 64.9677\n",
      "Epoch 211/500\n",
      "12/12 [==============================] - 0s 305us/sample - loss: 64.3623\n",
      "Epoch 212/500\n",
      "12/12 [==============================] - 0s 273us/sample - loss: 63.7623\n",
      "Epoch 213/500\n",
      "12/12 [==============================] - 0s 304us/sample - loss: 63.1680\n",
      "Epoch 214/500\n",
      "12/12 [==============================] - 0s 547us/sample - loss: 62.5791\n",
      "Epoch 215/500\n",
      "12/12 [==============================] - 0s 154us/sample - loss: 61.9960\n",
      "Epoch 216/500\n",
      "12/12 [==============================] - 0s 196us/sample - loss: 61.4181\n",
      "Epoch 217/500\n",
      "12/12 [==============================] - 0s 371us/sample - loss: 60.8456\n",
      "Epoch 218/500\n",
      "12/12 [==============================] - 0s 173us/sample - loss: 60.2784\n",
      "Epoch 219/500\n",
      "12/12 [==============================] - 0s 141us/sample - loss: 59.7165\n",
      "Epoch 220/500\n",
      "12/12 [==============================] - 0s 775us/sample - loss: 59.1599\n",
      "Epoch 221/500\n",
      "12/12 [==============================] - 0s 138us/sample - loss: 58.6084\n",
      "Epoch 222/500\n",
      "12/12 [==============================] - 0s 220us/sample - loss: 58.0622\n",
      "Epoch 223/500\n",
      "12/12 [==============================] - 0s 374us/sample - loss: 57.5209\n",
      "Epoch 224/500\n",
      "12/12 [==============================] - 0s 392us/sample - loss: 56.9848\n",
      "Epoch 225/500\n",
      "12/12 [==============================] - 0s 333us/sample - loss: 56.4537\n",
      "Epoch 226/500\n",
      "12/12 [==============================] - 0s 140us/sample - loss: 55.9273\n",
      "Epoch 227/500\n",
      "12/12 [==============================] - 0s 292us/sample - loss: 55.4061\n",
      "Epoch 228/500\n",
      "12/12 [==============================] - 0s 231us/sample - loss: 54.8895\n",
      "Epoch 229/500\n",
      "12/12 [==============================] - 0s 133us/sample - loss: 54.3780\n",
      "Epoch 230/500\n",
      "12/12 [==============================] - 0s 115us/sample - loss: 53.8712\n",
      "Epoch 231/500\n",
      "12/12 [==============================] - 0s 160us/sample - loss: 53.3690\n",
      "Epoch 232/500\n",
      "12/12 [==============================] - 0s 186us/sample - loss: 52.8715\n",
      "Epoch 233/500\n",
      "12/12 [==============================] - 0s 175us/sample - loss: 52.3787\n",
      "Epoch 234/500\n",
      "12/12 [==============================] - 0s 174us/sample - loss: 51.8905\n",
      "Epoch 235/500\n",
      "12/12 [==============================] - 0s 237us/sample - loss: 51.4068\n",
      "Epoch 236/500\n",
      "12/12 [==============================] - 0s 131us/sample - loss: 50.9277\n",
      "Epoch 237/500\n",
      "12/12 [==============================] - 0s 365us/sample - loss: 50.4529\n",
      "Epoch 238/500\n",
      "12/12 [==============================] - 0s 162us/sample - loss: 49.9827\n",
      "Epoch 239/500\n",
      "12/12 [==============================] - 0s 226us/sample - loss: 49.5168\n",
      "Epoch 240/500\n",
      "12/12 [==============================] - 0s 259us/sample - loss: 49.0552\n",
      "Epoch 241/500\n",
      "12/12 [==============================] - 0s 106us/sample - loss: 48.5980\n",
      "Epoch 242/500\n",
      "12/12 [==============================] - 0s 270us/sample - loss: 48.1450\n",
      "Epoch 243/500\n",
      "12/12 [==============================] - 0s 213us/sample - loss: 47.6963\n",
      "Epoch 244/500\n",
      "12/12 [==============================] - 0s 267us/sample - loss: 47.2517\n",
      "Epoch 245/500\n",
      "12/12 [==============================] - 0s 174us/sample - loss: 46.8112\n",
      "Epoch 246/500\n",
      "12/12 [==============================] - 0s 122us/sample - loss: 46.3749\n",
      "Epoch 247/500\n",
      "12/12 [==============================] - 0s 145us/sample - loss: 45.9427\n",
      "Epoch 248/500\n",
      "12/12 [==============================] - 0s 422us/sample - loss: 45.5144\n",
      "Epoch 249/500\n",
      "12/12 [==============================] - 0s 136us/sample - loss: 45.0902\n",
      "Epoch 250/500\n",
      "12/12 [==============================] - 0s 152us/sample - loss: 44.6698\n",
      "Epoch 251/500\n",
      "12/12 [==============================] - 0s 122us/sample - loss: 44.2534\n",
      "Epoch 252/500\n",
      "12/12 [==============================] - 0s 1ms/sample - loss: 43.8410\n",
      "Epoch 253/500\n",
      "12/12 [==============================] - 0s 173us/sample - loss: 43.4324\n",
      "Epoch 254/500\n",
      "12/12 [==============================] - 0s 584us/sample - loss: 43.0276\n",
      "Epoch 255/500\n",
      "12/12 [==============================] - 0s 173us/sample - loss: 42.6265\n",
      "Epoch 256/500\n",
      "12/12 [==============================] - 0s 198us/sample - loss: 42.2291\n",
      "Epoch 257/500\n",
      "12/12 [==============================] - 0s 174us/sample - loss: 41.8355\n",
      "Epoch 258/500\n",
      "12/12 [==============================] - 0s 145us/sample - loss: 41.4455\n",
      "Epoch 259/500\n",
      "12/12 [==============================] - 0s 211us/sample - loss: 41.0592\n",
      "Epoch 260/500\n",
      "12/12 [==============================] - 0s 276us/sample - loss: 40.6765\n",
      "Epoch 261/500\n",
      "12/12 [==============================] - 0s 126us/sample - loss: 40.2974\n",
      "Epoch 262/500\n",
      "12/12 [==============================] - 0s 144us/sample - loss: 39.9217\n",
      "Epoch 263/500\n",
      "12/12 [==============================] - 0s 139us/sample - loss: 39.5497\n",
      "Epoch 264/500\n",
      "12/12 [==============================] - 0s 139us/sample - loss: 39.1810\n",
      "Epoch 265/500\n",
      "12/12 [==============================] - 0s 140us/sample - loss: 38.8157\n",
      "Epoch 266/500\n",
      "12/12 [==============================] - 0s 139us/sample - loss: 38.4539\n",
      "Epoch 267/500\n",
      "12/12 [==============================] - 0s 142us/sample - loss: 38.0955\n",
      "Epoch 268/500\n",
      "12/12 [==============================] - 0s 175us/sample - loss: 37.7405\n",
      "Epoch 269/500\n",
      "12/12 [==============================] - 0s 103us/sample - loss: 37.3886\n",
      "Epoch 270/500\n",
      "12/12 [==============================] - 0s 139us/sample - loss: 37.0401\n",
      "Epoch 271/500\n",
      "12/12 [==============================] - 0s 186us/sample - loss: 36.6949\n",
      "Epoch 272/500\n",
      "12/12 [==============================] - 0s 142us/sample - loss: 36.3528\n",
      "Epoch 273/500\n",
      "12/12 [==============================] - 0s 141us/sample - loss: 36.0139\n",
      "Epoch 274/500\n",
      "12/12 [==============================] - 0s 136us/sample - loss: 35.6783\n",
      "Epoch 275/500\n",
      "12/12 [==============================] - 0s 114us/sample - loss: 35.3456\n",
      "Epoch 276/500\n",
      "12/12 [==============================] - 0s 174us/sample - loss: 35.0161\n",
      "Epoch 277/500\n",
      "12/12 [==============================] - 0s 146us/sample - loss: 34.6898\n",
      "Epoch 278/500\n",
      "12/12 [==============================] - 0s 158us/sample - loss: 34.3665\n",
      "Epoch 279/500\n",
      "12/12 [==============================] - 0s 107us/sample - loss: 34.0461\n",
      "Epoch 280/500\n",
      "12/12 [==============================] - 0s 159us/sample - loss: 33.7288\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 281/500\n",
      "12/12 [==============================] - 0s 222us/sample - loss: 33.4144\n",
      "Epoch 282/500\n",
      "12/12 [==============================] - 0s 441us/sample - loss: 33.1030\n",
      "Epoch 283/500\n",
      "12/12 [==============================] - 0s 138us/sample - loss: 32.7945\n",
      "Epoch 284/500\n",
      "12/12 [==============================] - 0s 126us/sample - loss: 32.4887\n",
      "Epoch 285/500\n",
      "12/12 [==============================] - 0s 127us/sample - loss: 32.1858\n",
      "Epoch 286/500\n",
      "12/12 [==============================] - 0s 116us/sample - loss: 31.8858\n",
      "Epoch 287/500\n",
      "12/12 [==============================] - 0s 143us/sample - loss: 31.5886\n",
      "Epoch 288/500\n",
      "12/12 [==============================] - 0s 115us/sample - loss: 31.2942\n",
      "Epoch 289/500\n",
      "12/12 [==============================] - 0s 238us/sample - loss: 31.0025\n",
      "Epoch 290/500\n",
      "12/12 [==============================] - 0s 370us/sample - loss: 30.7135\n",
      "Epoch 291/500\n",
      "12/12 [==============================] - 0s 151us/sample - loss: 30.4273\n",
      "Epoch 292/500\n",
      "12/12 [==============================] - 0s 316us/sample - loss: 30.1436\n",
      "Epoch 293/500\n",
      "12/12 [==============================] - 0s 338us/sample - loss: 29.8627\n",
      "Epoch 294/500\n",
      "12/12 [==============================] - 0s 160us/sample - loss: 29.5843\n",
      "Epoch 295/500\n",
      "12/12 [==============================] - 0s 452us/sample - loss: 29.3086\n",
      "Epoch 296/500\n",
      "12/12 [==============================] - 0s 167us/sample - loss: 29.0353\n",
      "Epoch 297/500\n",
      "12/12 [==============================] - 0s 159us/sample - loss: 28.7648\n",
      "Epoch 298/500\n",
      "12/12 [==============================] - 0s 107us/sample - loss: 28.4966\n",
      "Epoch 299/500\n",
      "12/12 [==============================] - 0s 452us/sample - loss: 28.2310\n",
      "Epoch 300/500\n",
      "12/12 [==============================] - 0s 620us/sample - loss: 27.9679\n",
      "Epoch 301/500\n",
      "12/12 [==============================] - 0s 230us/sample - loss: 27.7071\n",
      "Epoch 302/500\n",
      "12/12 [==============================] - 0s 394us/sample - loss: 27.4489\n",
      "Epoch 303/500\n",
      "12/12 [==============================] - 0s 188us/sample - loss: 27.1931\n",
      "Epoch 304/500\n",
      "12/12 [==============================] - 0s 118us/sample - loss: 26.9397\n",
      "Epoch 305/500\n",
      "12/12 [==============================] - 0s 442us/sample - loss: 26.6886\n",
      "Epoch 306/500\n",
      "12/12 [==============================] - 0s 113us/sample - loss: 26.4397\n",
      "Epoch 307/500\n",
      "12/12 [==============================] - 0s 303us/sample - loss: 26.1932\n",
      "Epoch 308/500\n",
      "12/12 [==============================] - 0s 140us/sample - loss: 25.9492\n",
      "Epoch 309/500\n",
      "12/12 [==============================] - 0s 180us/sample - loss: 25.7072\n",
      "Epoch 310/500\n",
      "12/12 [==============================] - 0s 841us/sample - loss: 25.4676\n",
      "Epoch 311/500\n",
      "12/12 [==============================] - 0s 174us/sample - loss: 25.2303\n",
      "Epoch 312/500\n",
      "12/12 [==============================] - 0s 176us/sample - loss: 24.9950\n",
      "Epoch 313/500\n",
      "12/12 [==============================] - 0s 487us/sample - loss: 24.7620\n",
      "Epoch 314/500\n",
      "12/12 [==============================] - 0s 324us/sample - loss: 24.5313\n",
      "Epoch 315/500\n",
      "12/12 [==============================] - 0s 149us/sample - loss: 24.3026\n",
      "Epoch 316/500\n",
      "12/12 [==============================] - 0s 148us/sample - loss: 24.0762\n",
      "Epoch 317/500\n",
      "12/12 [==============================] - 0s 335us/sample - loss: 23.8517\n",
      "Epoch 318/500\n",
      "12/12 [==============================] - 0s 188us/sample - loss: 23.6294\n",
      "Epoch 319/500\n",
      "12/12 [==============================] - 0s 514us/sample - loss: 23.4091\n",
      "Epoch 320/500\n",
      "12/12 [==============================] - 0s 151us/sample - loss: 23.1909\n",
      "Epoch 321/500\n",
      "12/12 [==============================] - 0s 118us/sample - loss: 22.9747\n",
      "Epoch 322/500\n",
      "12/12 [==============================] - 0s 115us/sample - loss: 22.7605\n",
      "Epoch 323/500\n",
      "12/12 [==============================] - 0s 117us/sample - loss: 22.5484\n",
      "Epoch 324/500\n",
      "12/12 [==============================] - 0s 357us/sample - loss: 22.3383\n",
      "Epoch 325/500\n",
      "12/12 [==============================] - 0s 292us/sample - loss: 22.1300\n",
      "Epoch 326/500\n",
      "12/12 [==============================] - 0s 179us/sample - loss: 21.9238\n",
      "Epoch 327/500\n",
      "12/12 [==============================] - 0s 142us/sample - loss: 21.7194\n",
      "Epoch 328/500\n",
      "12/12 [==============================] - 0s 138us/sample - loss: 21.5169\n",
      "Epoch 329/500\n",
      "12/12 [==============================] - 0s 269us/sample - loss: 21.3164\n",
      "Epoch 330/500\n",
      "12/12 [==============================] - 0s 127us/sample - loss: 21.1177\n",
      "Epoch 331/500\n",
      "12/12 [==============================] - 0s 151us/sample - loss: 20.9208\n",
      "Epoch 332/500\n",
      "12/12 [==============================] - 0s 143us/sample - loss: 20.7258\n",
      "Epoch 333/500\n",
      "12/12 [==============================] - 0s 210us/sample - loss: 20.5326\n",
      "Epoch 334/500\n",
      "12/12 [==============================] - 0s 154us/sample - loss: 20.3413\n",
      "Epoch 335/500\n",
      "12/12 [==============================] - 0s 228us/sample - loss: 20.1516\n",
      "Epoch 336/500\n",
      "12/12 [==============================] - 0s 231us/sample - loss: 19.9638\n",
      "Epoch 337/500\n",
      "12/12 [==============================] - 0s 212us/sample - loss: 19.7777\n",
      "Epoch 338/500\n",
      "12/12 [==============================] - 0s 172us/sample - loss: 19.5933\n",
      "Epoch 339/500\n",
      "12/12 [==============================] - 0s 119us/sample - loss: 19.4108\n",
      "Epoch 340/500\n",
      "12/12 [==============================] - 0s 135us/sample - loss: 19.2299\n",
      "Epoch 341/500\n",
      "12/12 [==============================] - 0s 138us/sample - loss: 19.0506\n",
      "Epoch 342/500\n",
      "12/12 [==============================] - 0s 167us/sample - loss: 18.8730\n",
      "Epoch 343/500\n",
      "12/12 [==============================] - 0s 143us/sample - loss: 18.6971\n",
      "Epoch 344/500\n",
      "12/12 [==============================] - 0s 162us/sample - loss: 18.5229\n",
      "Epoch 345/500\n",
      "12/12 [==============================] - 0s 108us/sample - loss: 18.3502\n",
      "Epoch 346/500\n",
      "12/12 [==============================] - 0s 167us/sample - loss: 18.1791\n",
      "Epoch 347/500\n",
      "12/12 [==============================] - 0s 267us/sample - loss: 18.0097\n",
      "Epoch 348/500\n",
      "12/12 [==============================] - 0s 126us/sample - loss: 17.8418\n",
      "Epoch 349/500\n",
      "12/12 [==============================] - 0s 145us/sample - loss: 17.6756\n",
      "Epoch 350/500\n",
      "12/12 [==============================] - 0s 144us/sample - loss: 17.5108\n",
      "Epoch 351/500\n",
      "12/12 [==============================] - 0s 131us/sample - loss: 17.3475\n",
      "Epoch 352/500\n",
      "12/12 [==============================] - 0s 172us/sample - loss: 17.1858\n",
      "Epoch 353/500\n",
      "12/12 [==============================] - 0s 247us/sample - loss: 17.0257\n",
      "Epoch 354/500\n",
      "12/12 [==============================] - 0s 170us/sample - loss: 16.8670\n",
      "Epoch 355/500\n",
      "12/12 [==============================] - 0s 135us/sample - loss: 16.7097\n",
      "Epoch 356/500\n",
      "12/12 [==============================] - 0s 145us/sample - loss: 16.5540\n",
      "Epoch 357/500\n",
      "12/12 [==============================] - 0s 143us/sample - loss: 16.3997\n",
      "Epoch 358/500\n",
      "12/12 [==============================] - 0s 124us/sample - loss: 16.2468\n",
      "Epoch 359/500\n",
      "12/12 [==============================] - 0s 88us/sample - loss: 16.0954\n",
      "Epoch 360/500\n",
      "12/12 [==============================] - 0s 99us/sample - loss: 15.9453\n",
      "Epoch 361/500\n",
      "12/12 [==============================] - 0s 152us/sample - loss: 15.7968\n",
      "Epoch 362/500\n",
      "12/12 [==============================] - 0s 143us/sample - loss: 15.6495\n",
      "Epoch 363/500\n",
      "12/12 [==============================] - 0s 139us/sample - loss: 15.5036\n",
      "Epoch 364/500\n",
      "12/12 [==============================] - 0s 856us/sample - loss: 15.3591\n",
      "Epoch 365/500\n",
      "12/12 [==============================] - 0s 319us/sample - loss: 15.2159\n",
      "Epoch 366/500\n",
      "12/12 [==============================] - 0s 254us/sample - loss: 15.0741\n",
      "Epoch 367/500\n",
      "12/12 [==============================] - 0s 146us/sample - loss: 14.9336\n",
      "Epoch 368/500\n",
      "12/12 [==============================] - 0s 629us/sample - loss: 14.7944\n",
      "Epoch 369/500\n",
      "12/12 [==============================] - 0s 877us/sample - loss: 14.6565\n",
      "Epoch 370/500\n",
      "12/12 [==============================] - 0s 132us/sample - loss: 14.5199\n",
      "Epoch 371/500\n",
      "12/12 [==============================] - 0s 446us/sample - loss: 14.3845\n",
      "Epoch 372/500\n",
      "12/12 [==============================] - 0s 242us/sample - loss: 14.2505\n",
      "Epoch 373/500\n",
      "12/12 [==============================] - 0s 207us/sample - loss: 14.1176\n",
      "Epoch 374/500\n",
      "12/12 [==============================] - 0s 161us/sample - loss: 13.9861\n",
      "Epoch 375/500\n",
      "12/12 [==============================] - 0s 167us/sample - loss: 13.8557\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 376/500\n",
      "12/12 [==============================] - 0s 168us/sample - loss: 13.7265\n",
      "Epoch 377/500\n",
      "12/12 [==============================] - 0s 366us/sample - loss: 13.5986\n",
      "Epoch 378/500\n",
      "12/12 [==============================] - 0s 224us/sample - loss: 13.4718\n",
      "Epoch 379/500\n",
      "12/12 [==============================] - 0s 207us/sample - loss: 13.3463\n",
      "Epoch 380/500\n",
      "12/12 [==============================] - 0s 277us/sample - loss: 13.2218\n",
      "Epoch 381/500\n",
      "12/12 [==============================] - 0s 200us/sample - loss: 13.0987\n",
      "Epoch 382/500\n",
      "12/12 [==============================] - 0s 127us/sample - loss: 12.9765\n",
      "Epoch 383/500\n",
      "12/12 [==============================] - 0s 279us/sample - loss: 12.8555\n",
      "Epoch 384/500\n",
      "12/12 [==============================] - 0s 254us/sample - loss: 12.7357\n",
      "Epoch 385/500\n",
      "12/12 [==============================] - 0s 220us/sample - loss: 12.6170\n",
      "Epoch 386/500\n",
      "12/12 [==============================] - 0s 201us/sample - loss: 12.4994\n",
      "Epoch 387/500\n",
      "12/12 [==============================] - 0s 241us/sample - loss: 12.3829\n",
      "Epoch 388/500\n",
      "12/12 [==============================] - 0s 310us/sample - loss: 12.2674\n",
      "Epoch 389/500\n",
      "12/12 [==============================] - 0s 149us/sample - loss: 12.1531\n",
      "Epoch 390/500\n",
      "12/12 [==============================] - 0s 441us/sample - loss: 12.0399\n",
      "Epoch 391/500\n",
      "12/12 [==============================] - 0s 209us/sample - loss: 11.9276\n",
      "Epoch 392/500\n",
      "12/12 [==============================] - 0s 198us/sample - loss: 11.8165\n",
      "Epoch 393/500\n",
      "12/12 [==============================] - 0s 354us/sample - loss: 11.7064\n",
      "Epoch 394/500\n",
      "12/12 [==============================] - 0s 191us/sample - loss: 11.5972\n",
      "Epoch 395/500\n",
      "12/12 [==============================] - 0s 334us/sample - loss: 11.4891\n",
      "Epoch 396/500\n",
      "12/12 [==============================] - 0s 180us/sample - loss: 11.3820\n",
      "Epoch 397/500\n",
      "12/12 [==============================] - 0s 235us/sample - loss: 11.2759\n",
      "Epoch 398/500\n",
      "12/12 [==============================] - 0s 707us/sample - loss: 11.1708\n",
      "Epoch 399/500\n",
      "12/12 [==============================] - 0s 233us/sample - loss: 11.0667\n",
      "Epoch 400/500\n",
      "12/12 [==============================] - 0s 137us/sample - loss: 10.9636\n",
      "Epoch 401/500\n",
      "12/12 [==============================] - 0s 167us/sample - loss: 10.8613\n",
      "Epoch 402/500\n",
      "12/12 [==============================] - 0s 300us/sample - loss: 10.7601\n",
      "Epoch 403/500\n",
      "12/12 [==============================] - 0s 154us/sample - loss: 10.6598\n",
      "Epoch 404/500\n",
      "12/12 [==============================] - 0s 268us/sample - loss: 10.5605\n",
      "Epoch 405/500\n",
      "12/12 [==============================] - 0s 166us/sample - loss: 10.4621\n",
      "Epoch 406/500\n",
      "12/12 [==============================] - 0s 218us/sample - loss: 10.3645\n",
      "Epoch 407/500\n",
      "12/12 [==============================] - 0s 228us/sample - loss: 10.2679\n",
      "Epoch 408/500\n",
      "12/12 [==============================] - 0s 270us/sample - loss: 10.1722\n",
      "Epoch 409/500\n",
      "12/12 [==============================] - 0s 144us/sample - loss: 10.0774\n",
      "Epoch 410/500\n",
      "12/12 [==============================] - 0s 289us/sample - loss: 9.9834\n",
      "Epoch 411/500\n",
      "12/12 [==============================] - 0s 2ms/sample - loss: 9.8903\n",
      "Epoch 412/500\n",
      "12/12 [==============================] - 0s 265us/sample - loss: 9.7981\n",
      "Epoch 413/500\n",
      "12/12 [==============================] - 0s 497us/sample - loss: 9.7069\n",
      "Epoch 414/500\n",
      "12/12 [==============================] - 0s 215us/sample - loss: 9.6164\n",
      "Epoch 415/500\n",
      "12/12 [==============================] - 0s 354us/sample - loss: 9.5268\n",
      "Epoch 416/500\n",
      "12/12 [==============================] - 0s 348us/sample - loss: 9.4380\n",
      "Epoch 417/500\n",
      "12/12 [==============================] - 0s 520us/sample - loss: 9.3499\n",
      "Epoch 418/500\n",
      "12/12 [==============================] - 0s 418us/sample - loss: 9.2628\n",
      "Epoch 419/500\n",
      "12/12 [==============================] - 0s 135us/sample - loss: 9.1765\n",
      "Epoch 420/500\n",
      "12/12 [==============================] - 0s 148us/sample - loss: 9.0910\n",
      "Epoch 421/500\n",
      "12/12 [==============================] - 0s 133us/sample - loss: 9.0062\n",
      "Epoch 422/500\n",
      "12/12 [==============================] - 0s 143us/sample - loss: 8.9222\n",
      "Epoch 423/500\n",
      "12/12 [==============================] - 0s 146us/sample - loss: 8.8391\n",
      "Epoch 424/500\n",
      "12/12 [==============================] - 0s 138us/sample - loss: 8.7567\n",
      "Epoch 425/500\n",
      "12/12 [==============================] - 0s 132us/sample - loss: 8.6751\n",
      "Epoch 426/500\n",
      "12/12 [==============================] - 0s 185us/sample - loss: 8.5942\n",
      "Epoch 427/500\n",
      "12/12 [==============================] - 0s 209us/sample - loss: 8.5141\n",
      "Epoch 428/500\n",
      "12/12 [==============================] - 0s 152us/sample - loss: 8.4347\n",
      "Epoch 429/500\n",
      "12/12 [==============================] - 0s 153us/sample - loss: 8.3561\n",
      "Epoch 430/500\n",
      "12/12 [==============================] - 0s 139us/sample - loss: 8.2782\n",
      "Epoch 431/500\n",
      "12/12 [==============================] - 0s 159us/sample - loss: 8.2011\n",
      "Epoch 432/500\n",
      "12/12 [==============================] - 0s 129us/sample - loss: 8.1247\n",
      "Epoch 433/500\n",
      "12/12 [==============================] - 0s 129us/sample - loss: 8.0489\n",
      "Epoch 434/500\n",
      "12/12 [==============================] - 0s 135us/sample - loss: 7.9739\n",
      "Epoch 435/500\n",
      "12/12 [==============================] - 0s 139us/sample - loss: 7.8996\n",
      "Epoch 436/500\n",
      "12/12 [==============================] - 0s 157us/sample - loss: 7.8259\n",
      "Epoch 437/500\n",
      "12/12 [==============================] - 0s 133us/sample - loss: 7.7529\n",
      "Epoch 438/500\n",
      "12/12 [==============================] - 0s 134us/sample - loss: 7.6807\n",
      "Epoch 439/500\n",
      "12/12 [==============================] - 0s 143us/sample - loss: 7.6091\n",
      "Epoch 440/500\n",
      "12/12 [==============================] - 0s 191us/sample - loss: 7.5382\n",
      "Epoch 441/500\n",
      "12/12 [==============================] - 0s 140us/sample - loss: 7.4679\n",
      "Epoch 442/500\n",
      "12/12 [==============================] - 0s 134us/sample - loss: 7.3983\n",
      "Epoch 443/500\n",
      "12/12 [==============================] - 0s 136us/sample - loss: 7.3294\n",
      "Epoch 444/500\n",
      "12/12 [==============================] - 0s 140us/sample - loss: 7.2610\n",
      "Epoch 445/500\n",
      "12/12 [==============================] - 0s 213us/sample - loss: 7.1933\n",
      "Epoch 446/500\n",
      "12/12 [==============================] - 0s 128us/sample - loss: 7.1263\n",
      "Epoch 447/500\n",
      "12/12 [==============================] - 0s 130us/sample - loss: 7.0599\n",
      "Epoch 448/500\n",
      "12/12 [==============================] - 0s 231us/sample - loss: 6.9941\n",
      "Epoch 449/500\n",
      "12/12 [==============================] - 0s 369us/sample - loss: 6.9289\n",
      "Epoch 450/500\n",
      "12/12 [==============================] - 0s 975us/sample - loss: 6.8643\n",
      "Epoch 451/500\n",
      "12/12 [==============================] - 0s 593us/sample - loss: 6.8003\n",
      "Epoch 452/500\n",
      "12/12 [==============================] - 0s 391us/sample - loss: 6.7370\n",
      "Epoch 453/500\n",
      "12/12 [==============================] - 0s 166us/sample - loss: 6.6741\n",
      "Epoch 454/500\n",
      "12/12 [==============================] - 0s 261us/sample - loss: 6.6119\n",
      "Epoch 455/500\n",
      "12/12 [==============================] - 0s 168us/sample - loss: 6.5503\n",
      "Epoch 456/500\n",
      "12/12 [==============================] - 0s 132us/sample - loss: 6.4893\n",
      "Epoch 457/500\n",
      "12/12 [==============================] - 0s 123us/sample - loss: 6.4288\n",
      "Epoch 458/500\n",
      "12/12 [==============================] - 0s 130us/sample - loss: 6.3688\n",
      "Epoch 459/500\n",
      "12/12 [==============================] - 0s 130us/sample - loss: 6.3095\n",
      "Epoch 460/500\n",
      "12/12 [==============================] - 0s 112us/sample - loss: 6.2507\n",
      "Epoch 461/500\n",
      "12/12 [==============================] - 0s 136us/sample - loss: 6.1924\n",
      "Epoch 462/500\n",
      "12/12 [==============================] - 0s 135us/sample - loss: 6.1347\n",
      "Epoch 463/500\n",
      "12/12 [==============================] - 0s 126us/sample - loss: 6.0775\n",
      "Epoch 464/500\n",
      "12/12 [==============================] - 0s 124us/sample - loss: 6.0208\n",
      "Epoch 465/500\n",
      "12/12 [==============================] - 0s 150us/sample - loss: 5.9647\n",
      "Epoch 466/500\n",
      "12/12 [==============================] - 0s 140us/sample - loss: 5.9091\n",
      "Epoch 467/500\n",
      "12/12 [==============================] - 0s 127us/sample - loss: 5.8540\n",
      "Epoch 468/500\n",
      "12/12 [==============================] - 0s 121us/sample - loss: 5.7995\n",
      "Epoch 469/500\n",
      "12/12 [==============================] - 0s 138us/sample - loss: 5.7454\n",
      "Epoch 470/500\n",
      "12/12 [==============================] - 0s 162us/sample - loss: 5.6919\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 471/500\n",
      "12/12 [==============================] - 0s 225us/sample - loss: 5.6388\n",
      "Epoch 472/500\n",
      "12/12 [==============================] - 0s 130us/sample - loss: 5.5862\n",
      "Epoch 473/500\n",
      "12/12 [==============================] - 0s 420us/sample - loss: 5.5341\n",
      "Epoch 474/500\n",
      "12/12 [==============================] - 0s 346us/sample - loss: 5.4826\n",
      "Epoch 475/500\n",
      "12/12 [==============================] - 0s 410us/sample - loss: 5.4315\n",
      "Epoch 476/500\n",
      "12/12 [==============================] - 0s 210us/sample - loss: 5.3808\n",
      "Epoch 477/500\n",
      "12/12 [==============================] - 0s 201us/sample - loss: 5.3307\n",
      "Epoch 478/500\n",
      "12/12 [==============================] - 0s 291us/sample - loss: 5.2810\n",
      "Epoch 479/500\n",
      "12/12 [==============================] - 0s 442us/sample - loss: 5.2317\n",
      "Epoch 480/500\n",
      "12/12 [==============================] - 0s 160us/sample - loss: 5.1830\n",
      "Epoch 481/500\n",
      "12/12 [==============================] - 0s 420us/sample - loss: 5.1347\n",
      "Epoch 482/500\n",
      "12/12 [==============================] - 0s 278us/sample - loss: 5.0869\n",
      "Epoch 483/500\n",
      "12/12 [==============================] - 0s 217us/sample - loss: 5.0394\n",
      "Epoch 484/500\n",
      "12/12 [==============================] - 0s 412us/sample - loss: 4.9925\n",
      "Epoch 485/500\n",
      "12/12 [==============================] - 0s 215us/sample - loss: 4.9459\n",
      "Epoch 486/500\n",
      "12/12 [==============================] - 0s 151us/sample - loss: 4.8998\n",
      "Epoch 487/500\n",
      "12/12 [==============================] - 0s 694us/sample - loss: 4.8541\n",
      "Epoch 488/500\n",
      "12/12 [==============================] - 0s 191us/sample - loss: 4.8089\n",
      "Epoch 489/500\n",
      "12/12 [==============================] - 0s 272us/sample - loss: 4.7641\n",
      "Epoch 490/500\n",
      "12/12 [==============================] - 0s 239us/sample - loss: 4.7197\n",
      "Epoch 491/500\n",
      "12/12 [==============================] - 0s 153us/sample - loss: 4.6757\n",
      "Epoch 492/500\n",
      "12/12 [==============================] - 0s 209us/sample - loss: 4.6321\n",
      "Epoch 493/500\n",
      "12/12 [==============================] - 0s 218us/sample - loss: 4.5889\n",
      "Epoch 494/500\n",
      "12/12 [==============================] - 0s 199us/sample - loss: 4.5462\n",
      "Epoch 495/500\n",
      "12/12 [==============================] - 0s 146us/sample - loss: 4.5038\n",
      "Epoch 496/500\n",
      "12/12 [==============================] - 0s 152us/sample - loss: 4.4618\n",
      "Epoch 497/500\n",
      "12/12 [==============================] - 0s 216us/sample - loss: 4.4202\n",
      "Epoch 498/500\n",
      "12/12 [==============================] - 0s 206us/sample - loss: 4.3790\n",
      "Epoch 499/500\n",
      "12/12 [==============================] - 0s 168us/sample - loss: 4.3382\n",
      "Epoch 500/500\n",
      "12/12 [==============================] - 0s 358us/sample - loss: 4.2977\n",
      "[[399.0087]]\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential([keras.layers.Dense(units=1, input_shape=[1])])\n",
    "model.compile(optimizer='sgd',loss='mean_squared_error')\n",
    "xs = np.array([1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 8.0, 9.0, 10.0,11.0, 12.0, 13.0], dtype=float)\n",
    "ys = np.array([100.0, 150.0, 200.0, 250.0, 300.0, 350.0, 450.0, 500.0, 550.0,600.0, 650.0,700.0], dtype=float)\n",
    "model.fit(xs,ys,epochs=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[399.0087]]\n"
     ]
    }
   ],
   "source": [
    "print(model.predict([7.0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 2 - Computer Vision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fashion MNIST\n",
    "fashion_mnist=keras.datasets.fashion_mnist\n",
    "(train_images,train_labels),(test_images,test_labels)=fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model= keras.Sequential([keras.layers.Flatten(input_shape=(28,28)),\n",
    "                        keras.layers.Dense(128,activation=tf.nn.relu),\n",
    "                        keras.layers.Dense(10,activation=tf.nn.softmax)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = tf.keras.datasets.fashion_mnist\n",
    "(training_images, training_labels), (test_images, test_labels) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n",
      "[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   1   0   0  13  73   0   0   1   4   0   0   0   0   1   1   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   3   0  36 136 127  62  54   0   0   0   1   3   4   0   0   3]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   6   0 102 204 176 134 144 123  23   0   0   0   0  12  10   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0 155 236 207 178 107 156 161 109  64  23  77 130  72  15]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   1   0  69 207 223 218 216 216 163 127 121 122 146 141  88 172  66]\n",
      " [  0   0   0   0   0   0   0   0   0   1   1   1   0 200 232 232 233 229 223 223 215 213 164 127 123 196 229   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0 183 225 216 223 228 235 227 224 222 224 221 223 245 173   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0 193 228 218 213 198 180 212 210 211 213 223 220 243 202   0]\n",
      " [  0   0   0   0   0   0   0   0   0   1   3   0  12 219 220 212 218 192 169 227 208 218 224 212 226 197 209  52]\n",
      " [  0   0   0   0   0   0   0   0   0   0   6   0  99 244 222 220 218 203 198 221 215 213 222 220 245 119 167  56]\n",
      " [  0   0   0   0   0   0   0   0   0   4   0   0  55 236 228 230 228 240 232 213 218 223 234 217 217 209  92   0]\n",
      " [  0   0   1   4   6   7   2   0   0   0   0   0 237 226 217 223 222 219 222 221 216 223 229 215 218 255  77   0]\n",
      " [  0   3   0   0   0   0   0   0   0  62 145 204 228 207 213 221 218 208 211 218 224 223 219 215 224 244 159   0]\n",
      " [  0   0   0   0  18  44  82 107 189 228 220 222 217 226 200 205 211 230 224 234 176 188 250 248 233 238 215   0]\n",
      " [  0  57 187 208 224 221 224 208 204 214 208 209 200 159 245 193 206 223 255 255 221 234 221 211 220 232 246   0]\n",
      " [  3 202 228 224 221 211 211 214 205 205 205 220 240  80 150 255 229 221 188 154 191 210 204 209 222 228 225   0]\n",
      " [ 98 233 198 210 222 229 229 234 249 220 194 215 217 241  65  73 106 117 168 219 221 215 217 223 223 224 229  29]\n",
      " [ 75 204 212 204 193 205 211 225 216 185 197 206 198 213 240 195 227 245 239 223 218 212 209 222 220 221 230  67]\n",
      " [ 48 203 183 194 213 197 185 190 194 192 202 214 219 221 220 236 225 216 199 206 186 181 177 172 181 205 206 115]\n",
      " [  0 122 219 193 179 171 183 196 204 210 213 207 211 210 200 196 194 191 195 191 198 192 176 156 167 177 210  92]\n",
      " [  0   0  74 189 212 191 175 172 175 181 185 188 189 188 193 198 204 209 210 210 211 188 188 194 192 216 170   0]\n",
      " [  2   0   0   0  66 200 222 237 239 242 246 243 244 221 220 193 191 179 182 182 181 176 166 168  99  58   0   0]\n",
      " [  0   0   0   0   0   0   0  40  61  44  72  41  35   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "np.set_printoptions(linewidth=200)\n",
    "plt.imshow(training_images[0])\n",
    "print(training_labels[0])\n",
    "print(training_images[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalizing\n",
    "training_images  = training_images / 255.0\n",
    "test_images = test_images / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([tf.keras.layers.Flatten(), \n",
    "                                    tf.keras.layers.Dense(128, activation=tf.nn.relu), \n",
    "                                    tf.keras.layers.Dense(10, activation=tf.nn.softmax)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sequential**: That defines a SEQUENCE of layers in the neural network\n",
    "\n",
    "**Flatten**: Remember earlier where our images were a square, when you printed them out? Flatten just takes that square and turns it into a 1 dimensional set.\n",
    "\n",
    "**Dense**: Adds a layer of neurons\n",
    "\n",
    "Each layer of neurons need an **activation function** to tell them what to do. There's lots of options, but just use these for now. \n",
    "\n",
    "**Relu** effectively means \"If X>0 return X, else return 0\" -- so what it does it it only passes values 0 or greater to the next layer in the network.\n",
    "\n",
    "**Softmax** takes a set of values, and effectively picks the biggest one, so, for example, if the output of the last layer looks like [0.1, 0.1, 0.05, 0.1, 9.5, 0.1, 0.05, 0.05, 0.05], it saves you from fishing through it looking for the biggest value, and turns it into [0,0,0,0,1,0,0,0,0] -- The goal is to save a lot of coding!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 8s 128us/sample - loss: 0.4987 - acc: 0.8248\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 6s 104us/sample - loss: 0.3749 - acc: 0.8648\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 6s 96us/sample - loss: 0.3372 - acc: 0.8765\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 5s 91us/sample - loss: 0.3106 - acc: 0.8862\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 6s 97us/sample - loss: 0.2943 - acc: 0.8923\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x13c41a320>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model.compile(optimizer = tf.optimizers.Adam(),\n",
    "#               loss = 'sparse_categorical_crossentropy',\n",
    "#               metrics=['accuracy'])\n",
    "\n",
    "# model.fit(training_images, training_labels, epochs=5)\n",
    "\n",
    "model.compile(optimizer = 'Adam',\n",
    "              loss = 'sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(training_images, training_labels, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 1s 53us/sample - loss: 0.3424 - acc: 0.87660s - loss: 0.3438 - acc: 0.876\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.34236203389167785, 0.8766]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(test_images, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Call back"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "回调函数是一组在训练的特定阶段被调用的函数集，你可以使用回调函数来观察训练过程中网络内部的状态和统计信息。通过传递回调函数列表到模型的.fit()中，即可在给定的训练阶段调用该函数集中的函数。\n",
    "\n",
    "【Tips】虽然我们称之为回调“函数”，但事实上Keras的回调函数是一个类，回调函数只是习惯性称呼"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 3 - Convolutional Neural Networks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Adding Fileter & Pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.13.1\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 26, 26, 64)        640       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 13, 13, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 11, 11, 64)        36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 5, 5, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 1600)              0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 128)               204928    \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 243,786\n",
      "Trainable params: 243,786\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/5\n",
      "60000/60000 [==============================] - 91s 2ms/sample - loss: 0.4506 - acc: 0.8368\n",
      "Epoch 2/5\n",
      "60000/60000 [==============================] - 92s 2ms/sample - loss: 0.2984 - acc: 0.8908\n",
      "Epoch 3/5\n",
      "60000/60000 [==============================] - 106s 2ms/sample - loss: 0.2538 - acc: 0.9060\n",
      "Epoch 4/5\n",
      "60000/60000 [==============================] - 89s 1ms/sample - loss: 0.2195 - acc: 0.9182\n",
      "Epoch 5/5\n",
      "60000/60000 [==============================] - 100s 2ms/sample - loss: 0.1945 - acc: 0.9279\n",
      "10000/10000 [==============================] - 4s 386us/sample - loss: 0.2654 - acc: 0.9036\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "mnist = tf.keras.datasets.fashion_mnist\n",
    "(training_images, training_labels), (test_images, test_labels) = mnist.load_data()\n",
    "training_images=training_images.reshape(60000, 28, 28, 1)\n",
    "training_images=training_images / 255.0\n",
    "test_images = test_images.reshape(10000, 28, 28, 1)\n",
    "test_images=test_images/255.0\n",
    "# 加入convolutional + max pooling\n",
    "model = tf.keras.models.Sequential([\n",
    "    # 第一层共64个3*3的filter，activation是relu，输入层是28*28*1，输出的28-3+1=26*26*64\n",
    "    tf.keras.layers.Conv2D(64,(3,3),activation='relu',input_shape=(28,28,1)),\n",
    "    # maxpooling层，每2*2个里面挑选max的出来，输出的是 13*13*64\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    # 再加一层，13-3+1=11*11*64\n",
    "    tf.keras.layers.Conv2D(64,(3,3),activation='relu',input_shape=(28,28,1)),\n",
    "    # 输出 5*5*64=1600\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    # 输出1600\n",
    "    tf.keras.layers.Flatten(),\n",
    "    # 输出128\n",
    "    tf.keras.layers.Dense(128,activation='relu'),\n",
    "    # 输出10\n",
    "    tf.keras.layers.Dense(10,activation='softmax'),  \n",
    "])\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "# 用来了解每一步的情况\n",
    "model.summary()\n",
    "model.fit(training_images, training_labels, epochs=5)\n",
    "test_loss = model.evaluate(test_images, test_labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Visualizing the Convolutions and Pooling\n",
    "\n",
    "This code will show us the convolutions graphically. The print (test_labels[;100]) shows us the first 100 labels in the test set, and you can see that the ones at index 0, index 23 and index 28 are all the same value (9). They're all shoes. Let's take a look at the result of running the convolution on each, and you'll begin to see common features between them emerge. Now, when the DNN is training on that data, it's working with a lot less, and it's perhaps finding a commonality between shoes based on this convolution/pooling combination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9 2 1 1 6 1 4 6 5 7 4 5 7 3 4 1 2 4 8 0 2 5 7 9 1 4 6 0 9 3 8 8 3 3 8 0 7\n",
      " 5 7 9 6 1 3 7 6 7 2 1 2 2 4 4 5 8 2 2 8 4 8 0 7 7 8 5 1 1 2 3 9 8 7 0 2 6\n",
      " 2 3 1 2 8 4 1 8 5 9 5 0 3 2 0 6 5 3 6 7 1 8 0 1 4 2]\n"
     ]
    }
   ],
   "source": [
    "print(test_labels[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 12 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "f, axarr = plt.subplots(3,4)\n",
    "FIRST_IMAGE=0\n",
    "SECOND_IMAGE=7\n",
    "THIRD_IMAGE=26\n",
    "CONVOLUTION_NUMBER = 1\n",
    "from tensorflow.keras import models\n",
    "layer_outputs = [layer.output for layer in model.layers]\n",
    "activation_model = tf.keras.models.Model(inputs = model.input, outputs = layer_outputs)\n",
    "for x in range(0,4):\n",
    "    f1 = activation_model.predict(test_images[FIRST_IMAGE].reshape(1, 28, 28, 1))[x]\n",
    "    axarr[0,x].imshow(f1[0, : , :, CONVOLUTION_NUMBER], cmap='inferno')\n",
    "    axarr[0,x].grid(False)\n",
    "    f2 = activation_model.predict(test_images[SECOND_IMAGE].reshape(1, 28, 28, 1))[x]\n",
    "    axarr[1,x].imshow(f2[0, : , :, CONVOLUTION_NUMBER], cmap='inferno')\n",
    "    axarr[1,x].grid(False)\n",
    "    f3 = activation_model.predict(test_images[THIRD_IMAGE].reshape(1, 28, 28, 1))[x]\n",
    "    axarr[2,x].imshow(f3[0, : , :, CONVOLUTION_NUMBER], cmap='inferno')\n",
    "    axarr[2,x].grid(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Exercise - Improving Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from os import path, getcwd, chdir\n",
    "\n",
    "# DO NOT CHANGE THE LINE BELOW. If you are developing in a local\n",
    "# environment, then grab mnist.npz from the Coursera Jupyter Notebook\n",
    "# and place it inside a local folder and edit the path to that location\n",
    "path = f\"{getcwd()}/mnist.npz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/kristy/Desktop/Phd/知识储备/Tensorflow'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "sess = tf.Session(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: train_mnist_conv\n",
    "def train_mnist_conv():\n",
    "    # Please write your code only where you are indicated.\n",
    "    # please do not remove model fitting inline comments.\n",
    "\n",
    "    # YOUR CODE STARTS HERE\n",
    "    class myCallback(tf.keras.callbacks.Callback):\n",
    "        def on_epoch_end(self, epoch, logs={}):\n",
    "            if(logs.get('acc')>0.998):\n",
    "                print(\"\\nReached 99.8% accuracy so cancelling training!\")\n",
    "                self.model.stop_training = True\n",
    "\n",
    "    callbacks = myCallback()\n",
    "    # YOUR CODE ENDS HERE\n",
    "\n",
    "    mnist = tf.keras.datasets.mnist\n",
    "    (training_images, training_labels), (test_images, test_labels) = mnist.load_data(path=path)\n",
    "    # YOUR CODE STARTS HERE\n",
    "    training_images=training_images.reshape(60000, 28, 28, 1)\n",
    "    training_images=training_images / 255.0\n",
    "    test_images = test_images.reshape(10000, 28, 28, 1)\n",
    "    test_images=test_images/255.0\n",
    "    # YOUR CODE ENDS HERE\n",
    "\n",
    "    model = tf.keras.models.Sequential([\n",
    "            # YOUR CODE STARTS HERE\n",
    "        tf.keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=(28, 28, 1)),\n",
    "        tf.keras.layers.MaxPooling2D(2, 2),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(128, activation='relu'),\n",
    "        tf.keras.layers.Dense(10, activation='softmax')\n",
    "            # YOUR CODE ENDS HERE\n",
    "    ])\n",
    "\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    # model fitting\n",
    "    history = model.fit(\n",
    "        # YOUR CODE STARTS HERE\n",
    "        training_images, training_labels, epochs=10, callbacks=[callbacks]   \n",
    "        # YOUR CODE ENDS HERE\n",
    "    ) \n",
    "    # model fitting\n",
    "    return history.epoch, history.history['acc'][-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 48s 795us/sample - loss: 0.1481 - acc: 0.9554\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 43s 715us/sample - loss: 0.0525 - acc: 0.9838\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 42s 700us/sample - loss: 0.0319 - acc: 0.9901\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 43s 715us/sample - loss: 0.0222 - acc: 0.9931\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 46s 774us/sample - loss: 0.0147 - acc: 0.9951\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 45s 757us/sample - loss: 0.0110 - acc: 0.9963\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 53s 881us/sample - loss: 0.0070 - acc: 0.9980\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 44s 736us/sample - loss: 0.0068 - acc: 0.9980\n",
      "Epoch 9/10\n",
      "59936/60000 [============================>.] - ETA: 0s - loss: 0.0052 - acc: 0.9985\n",
      "Reached 99.8% accuracy so cancelling training!\n",
      "60000/60000 [==============================] - 43s 722us/sample - loss: 0.0052 - acc: 0.9985\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([0, 1, 2, 3, 4, 5, 6, 7, 8], 0.9985)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_mnist_conv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 4 - Real World Image\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 区别与之前的model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    # 彩色照片*3\n",
    "    tf.keras.layers.Conv2D(16,(3,3),activation='relu',input_shape=(150,150,3)),\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    # 再加一层，13-3+1=11*11*64\n",
    "    tf.keras.layers.Conv2D(32,(3,3),activation='relu',input_shape=(28,28,1)),\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    # 再加一层\n",
    "    tf.keras.layers.Conv2D(64,(3,3),activation='relu',input_shape=(28,28,1)),\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "   \n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(512,activation='relu'),\n",
    "    # 如果是binary，可以用sigmoid function\n",
    "    tf.keras.layers.Dense(1,activation='sigmoid'),  \n",
    "])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
